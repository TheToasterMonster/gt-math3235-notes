\chapter{Discrete Random Variables}

\section{Probability Mass Functions}
\begin{example}
  Consider the following game: Flip a fair
  coin 10 times and roll a fair die. I give you
  \[
    (\text{number of heads}) \times (\text{number on die})
    \text{ dollars}.
  \]
  This is a simple game, but it is kind of
  painful to write in terms of events
  (e.g. $\PP(\text{win} \ge \$10))$).
  We would have to set
  \[
    \Omega = \{\text{all sequences like } (H, T, H, H, T, T, T, T, T, H, 4)\}
  \]
  and $\mathcal{F} = \mathcal{P}(\Omega)$. It is
  also not immediately obviously which
  sequences are in $\{\text{win} \ge \$10\}$. Instead,
  we would prefer something like
  \begin{quote}
    ``Let $H$ be the number of heads in 10
    fair coin tosses and let $R$ be the outcome
    of a roll of a fair die. Then you get $HR$ dollars.''
  \end{quote}
  How do we do this in our axiomatic framework?
  What are $H, R$? Here are some observations:
  \begin{itemize}
    \item $H, R$ are real numbers,
    \item and they are determined by the outcome of the
      experiment.
  \end{itemize}
  Thus we should think of $H, R$ as functions
  from $\Omega$ to $\R$. These are examples of
  \emph{discrete random variables}.
\end{example}

\begin{remark}
  The name ``random variable'' is just
  historic. Really, $H, R$ are non-random functions.
\end{remark}

\begin{remark}
  Can every function $X : \Omega \to \R$
  be a discrete random variable? Note that we want to
  talk about probabilities like
  $\PP(X = 17)$. This indicates that the event
  \[
    \{X = 17\} = \{\omega \in \Omega : X(\omega) = 17\}
  \]
  has to be in $\mathcal{F}$. So we require that
  $X$ is \emph{measurable}, i.e. for
  every $x \in \R$, we have
  $\{x \in \Omega : X(\omega) = x\} \in \mathcal{F}$.
  Also $H, R$ must have special properties,
  for instance they can only take on finitely many
  values.
\end{remark}

\begin{definition}
  A function $X : \Omega \to \R$ is a
  \emph{discrete random variable} if
  \begin{enumerate}[(i)]
    \item for every $x \in \R$, we have
      $\{X = x\} \in \mathcal{F}$,
    \item and $X(\Omega) = \{x \in \R : x = X(\omega) \text{ for some } \omega\}$
      is finite or countably infinite.
  \end{enumerate}
\end{definition}

\begin{remark}
  Often, we only care about what values $X$
  can take and with what probabilities. We store this
  data in a special function called the
  \emph{probability mass function}.
\end{remark}

\begin{definition}
  Let $X$ be a discrete random variable. Then
  its \emph{probability mass function (pmf)} is
  \[
    p_X : \R \to [0, 1] \quad \text{defined by} \quad p_X(s) = \PP(X = s).
  \]
\end{definition}

\begin{example}
  Let $X$ be the outcome of the roll of a fair die.
  Then
  \[
    p_X(s) = \begin{cases}
      1/6 & \text{if } s \in \{1, 2, 3, 4, 5, 6\}, \\
      0 & \text{otherwise}.
    \end{cases}
  \]
\end{example}

\begin{remark}
  Another sentence we want to say is:
  \begin{quote}
    ``A discrete random variable $X$ takes
    values $\{1, 7, 9\}$ with probabilities
    $1 / 2, 1 / 3, 1 / 6$, respectively if and only
    if
    \[
      p_X(s) =
      \begin{cases}
        1 / 2 & \text{if } s = 1, \\
        1 / 3 & \text{if } s = 7, \\
        1 / 6 & \text{if } s = 9, \\
        0 & \text{otherwise}.
      \end{cases}
    \]
  \end{quote}
  How do we know this exists? In other words,
  does there exist
  $(\Omega, \mathcal{F}, \PP)$ and
  $X : \Omega \to \R$ with this pmf?
\end{remark}

\begin{theorem}
  Let $S = \{s_i : i \in I\}$ be a countable
  subset of $\R$ and let
  $\{\pi_i : i \in I\}$ be a collection of
  numbers such that $\pi_i \ge 0$ and
  \[
    \sum_{i \in I} \pi_i = 1.
  \]
  Then there exists a probability space
  $(\Omega, \mathcal{F}, \PP)$ and a discrete
  random variable $X : \Omega \to \R$ such that
  \[
    p_X(s) =
    \begin{cases}
      \pi_i & \text{if } s = s_i, \\
      0 & \text{otherwise}.
    \end{cases}
  \]
\end{theorem}

\begin{proof}
  Take $\Omega = S$ and
  $\mathcal{F} = \mathcal{P}(S)$. Set
  \[\PP(A) = \sum_{i : s_i \in A} \pi_i\]
  and define $X : \Omega \to \R$ given by
  $X(\omega) = \omega$. Then one can check
  that $X$ has the desired pmf.
\end{proof}

\begin{remark}
  This allows us to just say
  \begin{quote}
    ``Let $X$ be a discrete random variable taking
    these values with these probabilities''
  \end{quote}
  without worrying about the underlying
  $(\Omega, \mathcal{F}, \PP)$.
\end{remark}

\section{Common Discrete Random Variables}
\begin{example}
  Some common examples of discrete random variables
  are:
  \begin{enumerate}
    \item \emph{Constant random variables}:
      Define $X : \Omega \to \R$ by
      $\omega \mapsto X(\omega) = C$.
    \item \emph{Bernoulli random variables}:
      For $0 < p < 1$, we say that
      $X \sim \mathrm{Ber}(p)$ if
      \[
        X =
        \begin{cases}
          1 & \text{with probability } p, \\
          0 & \text{with probability } q = 1 - p.
        \end{cases}
      \]
      This models a possibly unfair
      coin flip. The Bernoulli random variable $X$
      has pmf
      \[
        p_X(s) =
        \begin{cases}
          p & \text{if } s = 1, \\
          1 - p & \text{if } s = 0, \\
          0 & \text{otherwise}.
        \end{cases}
      \]
    \item \emph{Binomial random variables}:
      For $n \in \N^* = \N \setminus \{0\}$ and $0 < p < 1$, we say that
      $X \sim \mathrm{Bin}(n, p)$ if
      \[
        \PP(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}
      \]
      for $k = 0, 1, \dots, n$ and $\PP(X = k) = 0$
      otherwise. To that this is indeed a pmf,
      observe that
      \[
        \sum_{k = 0}^n \PP(X = k) = \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} = (p + (1 - p))^n = 1^n = 1.
      \]
      The $n = 1$ case reduces to a Bernoulli
      random variable.
    \item \emph{Geometric random variables}:
      For $0 < p < 1$, we say that
      $X \sim \mathrm{Geo}(p)$ if
      \[
        \PP(X = k) = p(1 - p)^{k - 1}
      \]
      for $k = 1, 2, 3, \dots$ and
      $\PP(X = k) = 0$ otherwise. The above function
      is clearly nonnegative and
      \[
        \sum_{k = 1}^\infty p(1 - p)^{k - 1}
        = \frac{p}{1 - (1 - p)} = 1,
      \]
      so this is indeed a pmf. The geometric
      random variable models the
      number of independent Bernoulli trials needed
      to obtain the first success.
  \end{enumerate}
\end{example}

\begin{example}
  Consider the random variable $X$ which counts
  the number of independent Bernoulli trials
  needed to get the 4th success. Note that the
  range of $X$ is $\{4, 5, 6, \dots\}$. Then
  \[
    \PP(X = k) = \binom{k - 1}{3} p^3(1 - p)^{k - 4} p
    = \binom{k - 1}{3} p^4 (1 - p)^{k - 4}
  \]
  for $k = 4, 5, 6, \dots$ and $\PP(X = k) = 0$
  otherwise.
  This is because
  the last trial must be a success and the
  previous $k - 1$ trials need to contain $3$
  successes. Here $X = \mathrm{NBin}(n = 4, p)$,
  the \emph{negative binomial random variable}. In general,
  $X \sim \mathrm{NBin}(n, p)$ takes on values
  $n, n + 1, n + 2, \dots$ and
  \[
    \PP(X = k) = \binom{k - 1}{n - 1} p^n (1 - p)^{k - n}
  \]
  for $k = n, n + 1, n + 2, \dots$. Note that
  the $n = 1$
  case reduces to a geometric random variable.
  The name
  comes from the binomial theorem with negative
  exponents.
\end{example}

\begin{example}
  We say that $X$ is a \emph{Poisson random variable}
  with parameter $\lambda > 0$, written
  $X \sim \mathrm{Poi}(\lambda)$, if $X$ takes the
  values $k = 0, 1, 2, \dots$ with probability mass
  function
  \[
    p_X(k) = \PP(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}.
  \]
  Note that $p_X$ is clearly nonnegative and
  \[
    \sum_{k = 0}^\infty p_X(k) = \sum_{k = 0}^\infty \frac{e^{-\lambda} \lambda^k}{k!}
    = e^{-\lambda} \sum_{k = 0}^\infty \frac{\lambda^k}{k!}
    = e^{-\lambda} e^\lambda = 1,
  \]
  so $p_X$ is indeed a pmf. One can view the
  Poisson random variable in the following manner:
  Suppose $X \sim \mathrm{Bin}(n, p)$ with
  $n \gg 1$ and $p \ll 1$, e.g. $n = 10^5$ and
  $p = 10^{-4}$. Then
  \[
    \PP(X = 100) =
    \binom{10^5}{100} \left(\frac{1}{10^4}\right)^{100}
    \left(1 - \frac{1}{10^4}\right)^{10^5 - 100}.
  \]
  This is very difficult to compute.
  Instead, we approximate this via the Poisson
  random variable.
\end{example}

\begin{prop}
  Let $n \to \infty$ and $p = p(n) \to 0$ in such
  a way that $np(n) \to \lambda > 0$ as $n \to \infty$.
  Then
  \[
    \binom{n}{k} p^k (1 - p)^{n - k} \xrightarrow[n \to \infty]{} \frac{e^{-\lambda} \lambda^k}{k!},
  \]
  i.e. $p_X(k) \to p_Y(k)$ pointwise for
  $k = 0, 1, 2, \dots$, where $X \sim \mathrm{Bin}(n, p)$
  and $Y \sim \mathrm{Poi}(\lambda)$.
\end{prop}

\begin{proof}
  Observe that
  \begin{align*}
    \binom{n}{k} p^k (1 - p)^{n - k}
    &= \frac{n!}{k!(n - k)!} p^k (1 - p)^{n - k}
    = \frac{1}{k!} \left[n(n - 1) \dots (n - k + 1)p^k(1 - p)^{-k} (1 - p)^n\right] \\
    &= \frac{1}{k!} \left[\frac{n(n - 1) \dots (n - k + 1)}{n^k} n^k p^k (1 - p)^{-k} (1 - p)^n\right].
  \end{align*}
  Now notice that $n^k p^k = (np)^k \to \lambda^k$
  since $np \to \lambda$, and
  \[
    \lim_{n \to \infty} \frac{n(n - 1) \dots (n - k + 1)}{n^k} = 1
    \quad \text{and} \quad
    \lim_{p \to 0} (1 - p)^{-k} = 1.
  \]
  Finally, setting $\lambda = np$,
  \[
    (1 - p)^n =
    \left(1 - \frac{\lambda}{n}\right)^n
    \to e^{-\lambda}.
  \]
  Putting all of this together, we see that
  \[
    \binom{n}{k} p^k (1 - p)^{n - k} \xrightarrow[n \to \infty]{} \frac{e^{-\lambda} \lambda^k}{k!},
  \]
  which is the desired result.
\end{proof}

\section{Expectation of Random Variables}
\begin{remark}
  Suppose $X : \Omega \to \R$ is a discrete random variable and
  $h : \R \to \R$. Then we have:
  \[
    \begin{tikzcd}
      \Omega \ar[r, "X"] \ar[dr, "h(X)", swap] & \R \ar[d, "h"] \\
      & \R
    \end{tikzcd}
  \]
  In particular, $h \circ X : \Omega \to \R$ is
  also a random variable.
\end{remark}

\begin{definition}
  Let $X$ be a discrete random variable. The
  \emph{(mathematical) expectation} of $X$ is\footnote{We write $\mathcal{R}(X)$ to denote the range of $X$.}
  \[
    \EE[X] = \sum_{x \in \mathcal{R}(X)} x p_X(x)
  \]
  if the above sum exists and converges
  absolutely,\footnote{i.e. $\sum_{x \in \mathcal{R}(X)} |x| p_X(x) < \infty$.}
  where
  $p_X$ is the probability mass function of $X$.
\end{definition}

\begin{remark}
  When $X$ is discrete, the expectation coincides
  with the usual notion of a mean. In general,
  the expectation is some kind of weighted mean.
\end{remark}

\begin{remark}
  Observe that the sum in the definition of
  $\EE[X]$ need not converge. Even worse, if it
  only converges conditionally, then by the Riemann
  rearrangement theorem we may get any real value
  we wish by reordering the sum. This is why we
  require absolute convergence.
\end{remark}

\begin{example}
  Set $Y = X^2$. Then we have
  \[
    \EE[X^2] = \EE[Y] = \sum_{y \in \mathcal{R}(Y)} y \PP(Y = y)
    = \sum_{y \in \mathcal{R}(X^2)} y \PP(X^2 = y).
  \]
  If we explicitly let
  \[
    X =
    \begin{cases}
      1 & \text{with probability } 1 / 2, \\
      -1 & \text{with probability } 1 / 2,
    \end{cases}
  \]
  we see that $\EE[X] = 0$. We can also see that
  $\EE[X^2] = 1$ since $X^2 = 1$ with probability $1$.
  Equivalently, we can compute that
  \[
    \EE[X^2]
    = \sum_{y \in \mathcal{R}(X^2)} y \PP(X^2 = y)
    = 1 \cdot \PP(X^2 = 1) = 1.
  \]
\end{example}

\begin{prop}[Law of the unconcious statistician]
  For any $h : \R \to \R$ and $X : \Omega \to \R$
  discrete,
  \[
    \EE[h(X)] = \sum_{x \in \mathcal{R}(X)} h(x) p_X(x)
  \]
  where $p_X$ is the pmf of $X$, provided
  these sums exist and converge absolutely.
\end{prop}

\begin{proof}
  Let $Y = h(X)$. Then we have
  \[
    \EE[h(X)] = \EE[Y]
    = \sum_{y \in \mathcal{R}(Y)} y p_Y(y)
    = \sum_{y \in \mathcal{R}(Y)} y \PP(h(X) = y)
    = \sum_{x \in \mathcal{R}(X)} h(x) \PP(h(X) = y).
  \]
  Note that that $y$ in the last term is
  $h(x)$, and thus $\PP(h(X) = y) = \PP(h(X) = h(x)) = p_X(x)$.
  Then
  \[
    \EE[h(X)] = \sum_{x \in \mathcal{R}(X)} h(x) p_X(x),
  \]
  which is precisely the desired result.
\end{proof}

\begin{remark}
  In the discrete case, we do not require
  that $h$ be measurable
  since $\mathcal{R}(X)$ is at most countable.
\end{remark}

\begin{prop}
  We have the following properties of expectation:
  \begin{enumerate}[(i)]
    \item If $X \ge 0$, then $\EE[X] \ge 0$.
    \item If $X = C$ is constant, then
      $\EE[X] = C$.
    \item If $\EE[aX + bY] = a\EE[X] + b\EE[Y]$.
  \end{enumerate}
\end{prop}

\begin{proof}
  $(i)$ Since $X \ge 0$, we have $\mathcal{R}(X) \subseteq [0, \infty)$
  and thus
  \[
    \EE[X] = \sum_{x \in \mathcal{R}(X)} x p_X(x) \ge 0
  \]
  since every term in the sum is nonnegative.

  $(ii)$ Since $\mathcal{R}(X) = \{C\}$, we have
  $\PP(X = C) = 1$ and thus
  \[
    \EE[X] = \sum_{x \in \mathcal{R}(X)} x p_X(x)
    = C \cdot \PP(X = C) = C.
  \]
  This is the desired result.

  $(iii)$ We compute that
  \begin{align*}
    \EE[aX + bY]
    &= \sum_{ax + by \in \mathcal{R}(aX + bY)} (ax + by) p_{aX + bY}(ax + by)
    = \sum_{x \in \mathcal{R}(X)} ax p_{X}(x)
    + \sum_{y \in \mathcal{R}(Y)} by p_{Y}(y) \\
    &= a \sum_{x \in \mathcal{R}(X)} x p_{X}(x)
    + b \sum_{y \in \mathcal{R}(Y)} y p_{Y}(y)
    = a\EE[X] + b\EE[Y],
  \end{align*}
  which is the desired equality.
\end{proof}

\begin{example} We compute the following:
  \begin{enumerate}
    \item Let $X \sim \mathrm{Ber}(p)$. Then
      $\EE[X] = 0(1 - p) + 1p = p$.
    \item Let $X \sim \mathrm{Bin}(n, p)$. Then
      \begin{align*}
        \EE[X]
        &= \sum_{k = 0}^n k \binom{n}{k} p^k (1 - p)^{n - k}
        = \sum_{k = 1}^n k \binom{n}{k} p^k (1 - p)^{n - k}
        = \sum_{k = 1}^n k \frac{n!}{k!(n - k)!} p^k (1 - p)^{n - k} \\
        &= \sum_{k = 1}^n \frac{n!}{(k - 1)!(n - k)!} p^k (1 - p)^{n - k}
        = n \sum_{k = 1}^n \frac{(n - 1)!}{(k - 1)!(n - k)!} p^k (1 - p)^{n - k} \\
        &= n \sum_{k = 1}^n \binom{n - 1}{k - 1} p^k (1 - p)^{n - k}
        = np \sum_{k = 1}^n \binom{n - 1}{k - 1} p^{k - 1} (1 - p)^{n - 1 - (k - 1)} = np.
      \end{align*}
      In the last step we re-index with $j = k - 1$,
      and then recognize the terms as the pmf
      of a $\mathrm{Bin}(n - 1, p)$ random variable,
      which must sum to $1$ over $0 \le j \le n - 1$.
    \item Let $X \sim \mathrm{Geo}(p)$. Then
      \begin{align*}
        \EE[X]
        &= \sum_{k = 1}^\infty k p(1 - p)^{k - 1}
        = p \sum_{k = 1}^\infty k(1 - p)^{k - 1}
        = p \sum_{k = 1}^\infty \frac{d}{dx}\left. (1 - x)^k\right|_{x = p}
        = p \frac{d}{dx} \left.\sum_{k = 1}^\infty (1 - x)^k\right|_{x = p} \\
        &= p \frac{d}{dx} \left.\frac{1 - x}{1 - (1 - x)}\right|_{x = p}
        = p \frac{d}{dx} \left.\frac{1 - x}{x}\right|_{x = p}
        = p \frac{d}{dx} \left.\left(1 - \frac{1}{x}\right)\right|_{x = p}
        = p \cdot \frac{1}{p^2} = p.
      \end{align*}
      The exchange of the sum and derivative is
      justified since $0 < p < 1$, so we are in the
      region of uniform convergence of the power
      series.
  \end{enumerate}
\end{example}

\section{Moments}
Recall that by the law of the unconscious statistician,
we have $\EE[X^2] = \sum_{x \in \mathcal{R}(X)} x^2 p_X(x)$.
More generally,
\[
  \EE[X^k] = \sum_{x \in \mathcal{R}(X)} x^k p_X(x)
\]
for $k \ge 1$, provided this series converges
absolutely.

\begin{definition}
  For a random variable $X$,
  \begin{itemize}
    \item $\EE[X^k]$ is called the \emph{moment of order $k$} of $X$,
    \item $\EE[|X|^k]$ is called the \emph{absolute moment of order $k$} of $X$,
    \item $\EE[(X - \EE[X])^k]$ is called the \emph{centered moment of order $k$} of $X$.
    \item and $\EE[|X - \EE[X]|^k]$ is called the \emph{centered absolute moment of order $k$} of $X$.
  \end{itemize}
\end{definition}

\section{Variance}
\begin{definition}
  Let $X$ be a random variable with finite
  $2$nd moment, i.e. we have $\EE[X^2] < \infty$. Then
  the \emph{variance} of $X$ is given by
  \[
    \Var[X] = \EE[(X - \EE[X])^2].
  \]
\end{definition}

\begin{example}
  Define the random variables $X = 0$,
  \[
    Y =
    \begin{cases}
      1 & \text{with probability } 1 / 2, \\
      -1 & \text{with probability } 1 / 2,
    \end{cases}
    \quad \text{and} \quad
    Z =
    \begin{cases}
      10 & \text{with probability } 1 / 2, \\
      -10 & \text{with probability } 1 / 2.
    \end{cases}
  \]
  Then $\EE[X] = \EE[Y] = \EE[Z] = 0$. But observe
  that we have
  \begin{align*}
    \Var[X] &= \EE[(X - \EE[X])^2] = \EE[X^2] = 0, \\
    \Var[Y] &= \EE[(Y - \EE[Y])^2] = \EE[Y^2] = 1, \\
    \Var[Z] &= \EE[(Z - \EE[Z])^2] = \EE[Z^2] = 100,
  \end{align*}
  which are not the same.
\end{example}

\begin{remark}
  The variance is a measure of the spread of a
  random variable about its mean.
\end{remark}

\begin{definition}
  The positive square root of $\Var[X]$ is
  called the \emph{standard deviation} of $X$.
\end{definition}

\begin{prop}
  We have the following properties of variance:
  \begin{enumerate}[(i)]
    \item $\Var[\alpha X] = \alpha^2 \Var[X]$
      for all $\alpha \in \R$,
    \item $\Var[X] = \EE[X^2] - (\EE[X])^2$,
    \item $\Var[X] = \EE[X(X - 1)] + \EE[X] - (\EE[X])^2$,
    \item and $\Var[X] = 0$ if and only if
      $X$ is constant.
  \end{enumerate}
\end{prop}

\begin{proof}
  $(i)$ We can compute that
  \[
    \Var[\alpha X] = \EE[(\alpha X - \EE[\alpha X])^2]
    = \EE[\alpha^2 (X - \EE[X])^2]
    = \alpha^2 \EE[(X - \EE[X])^2]
    = \alpha^2 \Var[X],
  \]
  by the linearity of expectation.

  $(ii)$ Again by the linearity of expectation, we have
  \begin{align*}
    \Var[X] = \EE[(X - \EE[X])^2]
    &= \EE[X^2 - 2X\EE[X] + (\EE[X])^2] \\
    &= \EE[X^2] - 2\EE[X \EE[X]] + \EE[(\EE[X])^2] \\
    &= \EE[X^2] - 2\EE[X] \EE[X] + (\EE[X])^2
  \end{align*}
  since $\EE[X]$ and $(\EE[X])^2$ are constants.

  $(iii)$ Simply write
  $\EE[X(X - 1)] = \EE[X^2 - X] = \EE[X^2] - \EE[X]$
  and apply $(ii)$.

  $(iv)$ $(\Leftarrow)$ If $X$ is constant, then
  $X = \EE[X]$, so
  \[
    \Var[X] = \EE[(X - \EE[X])^2] = \EE[(\EE[X] - \EE[X])^2]
    = \EE[0] = 0.
  \]
    $(\Rightarrow)$ Suppose $\Var[X] = 0$. Then we find that
    \[
      0 = \Var[X]
      = \sum_{x \in \mathcal{R}(X)} (x - \EE[X])^2 p_X(x).
    \]
    This is a sum of nonnegative terms, so
    each term must be zero, i.e. $(x - \EE[X])^2 p_X(x) = 0$.
    Since $x \in \mathcal{R}(X)$, we must have
    $p_X(x) > 0$, and thus $(x - \EE[X])^2 = 0$.
    This gives $x = \EE[X]$ for every $x \in \mathcal{R}(X)$,
    so we conclude that $X = \EE[X]$ must be constant.
\end{proof}

\begin{exercise}
  Compute the variance for the following random
  variables:
  \begin{enumerate}[(i)]
    \item $X \sim \mathrm{Ber}(p)$.
      The answer should be $\Var[X] = p(1 - p)$.
    \item $X \sim \mathrm{Bin}(n, p)$.
      The answer should be $\Var[X] = np(1 - p)$.
    \item $X \sim \mathrm{Poi}(\lambda)$.
      The answer should be $\Var[X] = \lambda$.
    \item $X \sim \mathrm{Geo}(p)$.
      We know $\EE[X] = 1 / p$, what is $\Var[X]$?
    \item $X \sim \mathrm{NBin}(r, p)$.
      We know $\EE[X] = r / p$, what is
      $\Var[X]$?
  \end{enumerate}
\end{exercise}

\section{Conditional Expectation}
\begin{definition}
  Let $X$ be a random variable and let
  $B$ be an event such that $\PP(B) > 0$.
  Then the \emph{conditional expectation} of $X$ given
  $B$ is defined by
  \[
    \EE[X | B]
    = \sum_{x \in \mathcal{R}(X)} x \PP(X = x | B)
    = \sum_{x \in \mathcal{R}(X)} \frac{x \PP(\{X = x\} \cap B)}{\PP(B)}.
  \]
\end{definition}

\begin{remark}
  Recall that $\{B_i\}_{i = 1}^\infty$ is
  a partition of $\Omega$ if the $B_i$
  are pairwise disjoint events and
  $\Omega = \bigcup_{i = 1}^\infty B_i$.
\end{remark}

\begin{theorem}[Partition theorem in expectation]
  Let $X$ be a discrete random variable and
  $\{B_k\}_{k = 1}^\infty$ be a partition of
  $\Omega$ with $\PP(B_k) > 0$ for each $k \ge 1$.
  Then
  \[
    \EE[X] = \sum_{k = 1}^\infty \EE[X | B_k] \PP(B_k).
  \]
\end{theorem}

\begin{proof}
  Use the definition of conditional expectation to
  write
  \begin{align*}
    \sum_{k = 1}^\infty \EE[X | B_k] \PP(B_k)
    = \sum_{k = 1}^\infty \sum_{x \in \mathcal{R}(X)} x \PP(X = x | B_k) \PP(B_k)
    &= \sum_{x \in \mathcal{R}(X)} x \sum_{k = 1}^\infty  \PP(X = x | B_k) \PP(B_k) \\
    &=  \sum_{x \in \mathcal{R}(X)} x \PP(X = x)
    = \EE[X]
  \end{align*}
  by the usual partition theorem. Exchanging
  the sums is permissible by absolute convergence
  of $\EE[X]$.
\end{proof}

\begin{remark}
  One can see this as saying
  ``the expectation of the conditional expectation
  is the expectation.''
\end{remark}

\begin{example}
  Suppose a coin flips heads with probability $p$
  and tails with probability $1 - p$. What is the
  expected length of the initial run (of consecutive
  heads if
  the first flip is heads, or of consecutive
  tails if the first flip is tails)? Let $X$ be the
  length of the initial run and $H$ be the event that
  the first flip is heads. Then
  \[
    \PP(X = k | H) = p^{k - 1}(1 - p)
    \quad \text{for $k = 1, 2, \dots$.}
  \]
  Similarly we find
  \[
    \PP(X = k | H^c) = (1 - p)^{k - 1}p
    \quad \text{for $k = 1, 2, \dots$.}
  \]
  Since $\{H, H^c\}$ is a partition of $\Omega$,
  we can use the partition theorem in expectation
  to write
  \[
    \EE[X] = \EE[X | H] \PP(H) + \EE[X | H^c] \PP(H^c). \tag{$*$}
  \]
  We can compute that
  \[
    \EE[X | H] = \sum_{x \in \mathcal{R}(X)} x \PP(X = x | H)
    = \sum_{k = 1}^\infty k p^{k - 1}(1 - p)
    = (1 - p) \sum_{k = 1}^\infty k p^{k - 1}
    = \frac{1 - p}{(1 - p)^2} = \frac{1}{1 - p}.
  \]
  Similarly we can find
  \[
    \EE[X | H^c] = \sum_{x \in \mathcal{R}(X)} x \PP(X = x | H^c)
    = \sum_{k = 1}^\infty k (1 - p)^{k - 1}p
    = p \sum_{k = 1}^\infty k (1 - p)^{k - 1}
    = \frac{p}{p^2} = \frac{1}{p}.
  \]
  Thus by substituting these values into $(*)$ we
  obtain
  \[
    \EE[X] = \frac{1}{1 - p} \cdot p + \frac{1}{p} \cdot (1 - p)
    = \frac{1}{p(1 - p)} - 2.
  \]
  In particular, if $p = 1 / 2$, then we find that
  $\EE[X] = 2$.
\end{example}

\section{Homework Problems}
Problems \#1, 2, 4, 5, 6, 7, 9, 10 from Grimmett and Welsh.
