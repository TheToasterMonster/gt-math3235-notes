\chapter{Discrete Random Variables}

\section{Probability Mass Functions}
\begin{example}
  Consider the following game: Flip a fair
  coin 10 times and roll a fair die. I give you
  \[
    (\text{number of heads}) \times (\text{number on die})
    \text{ dollars}.
  \]
  This is a simple game, but it is kind of
  painful to write in terms of events
  (e.g. $\PP(\text{win} \ge \$10))$).
  We would have to set
  \[
    \Omega = \{\text{all sequences like } (H, T, H, H, T, T, T, T, T, H, 4)\}
  \]
  and $\mathcal{F} = \mathcal{P}(\Omega)$. It is
  also not immediately obviously which
  sequences are in $\{\text{win} \ge \$10\}$. Instead,
  we would prefer something like
  \begin{quote}
    ``Let $H$ be the number of heads in 10
    fair coin tosses and let $R$ be the outcome
    of a roll of a fair die. Then you get $HR$ dollars.''
  \end{quote}
  How do we do this in our axiomatic framework?
  What are $H, R$? Here are some observations:
  \begin{itemize}
    \item $H, R$ are real numbers,
    \item and they are determined by the outcome of the
      experiment.
  \end{itemize}
  Thus we should think of $H, R$ as functions
  from $\Omega$ to $\R$. These are examples of
  \emph{discrete random variables}.
\end{example}

\begin{remark}
  The name ``random variable'' is just
  historic. Really, $H, R$ are non-random functions.
\end{remark}

\begin{remark}
  Can every function $X : \Omega \to \R$
  be a discrete random variable? Note that we want to
  talk about probabilities like
  $\PP(X = 17)$. This indicates that the event
  \[
    \{X = 17\} = \{\omega \in \Omega : X(\omega) = 17\}
  \]
  has to be in $\mathcal{F}$. So we require that
  $X$ is \emph{measurable}, i.e. for
  every $x \in \R$, we have
  $\{x \in \Omega : X(\omega) = x\} \in \mathcal{F}$.
  Also $H, R$ must have special properties,
  for instance they can only take on finitely many
  values.
\end{remark}

\begin{definition}
  A function $X : \Omega \to \R$ is a
  \emph{discrete random variable} if
  \begin{enumerate}[(i)]
    \item for every $x \in \R$, we have
      $\{X = x\} \in \mathcal{F}$,
    \item and $X(\Omega) = \{x \in \R : x = X(\omega) \text{ for some } \omega\}$
      is finite or countably infinite.
  \end{enumerate}
\end{definition}

\begin{remark}
  Often, we only care about what values $X$
  can take and with what probabilities. We store this
  data in a special function called the
  \emph{probability mass function}.
\end{remark}

\begin{definition}
  Let $X$ be a discrete random variable. Then
  its \emph{probability mass function (pmf)} is
  \[
    p_X : \R \to [0, 1] \quad \text{defined by} \quad p_X(s) = \PP(X = s).
  \]
\end{definition}

\begin{example}
  Let $X$ be the outcome of the roll of a fair die.
  Then
  \[
    p_X(s) = \begin{cases}
      1/6 & \text{if } s \in \{1, 2, 3, 4, 5, 6\}, \\
      0 & \text{otherwise}.
    \end{cases}
  \]
\end{example}

\begin{remark}
  Another sentence we want to say is:
  \begin{quote}
    ``A discrete random variable $X$ takes
    values $\{1, 7, 9\}$ with probabilities
    $1 / 2, 1 / 3, 1 / 6$, respectively if and only
    if
    \[
      p_X(s) =
      \begin{cases}
        1 / 2 & \text{if } s = 1, \\
        1 / 3 & \text{if } s = 7, \\
        1 / 6 & \text{if } s = 9, \\
        0 & \text{otherwise}.
      \end{cases}
    \]
  \end{quote}
  How do we know this exists? In other words,
  does there exist
  $(\Omega, \mathcal{F}, \PP)$ and
  $X : \Omega \to \R$ with this pmf?
\end{remark}

\begin{theorem}
  Let $S = \{s_i : i \in I\}$ be a countable
  subset of $\R$ and let
  $\{\pi_i : i \in I\}$ be a collection of
  numbers such that $\pi_i \ge 0$ and
  \[
    \sum_{i \in I} \pi_i = 1.
  \]
  Then there exists a probability space
  $(\Omega, \mathcal{F}, \PP)$ and a discrete
  random variable $X : \Omega \to \R$ such that
  \[
    p_X(s) =
    \begin{cases}
      \pi_i & \text{if } s = s_i, \\
      0 & \text{otherwise}.
    \end{cases}
  \]
\end{theorem}

\begin{proof}
  Take $\Omega = S$ and
  $\mathcal{F} = \mathcal{P}(S)$. Set
  \[\PP(A) = \sum_{i : s_i \in A} \pi_i\]
  and define $X : \Omega \to \R$ given by
  $X(\omega) = \omega$. Then one can check
  that $X$ has the desired pmf.
\end{proof}

\begin{remark}
  This allows us to just say
  \begin{quote}
    ``Let $X$ be a discrete random variable taking
    these values with these probabilities''
  \end{quote}
  without worrying about the underlying
  $(\Omega, \mathcal{F}, \PP)$.
\end{remark}

\begin{example}
  Some common examples of discrete random variables
  are:
  \begin{enumerate}
    \item \emph{Constant random variables}:
      Define $X : \Omega \to \R$ by
      $\omega \mapsto X(\omega) = C$.
    \item \emph{Bernoulli random variables}:
      For $0 < p < 1$, we say that
      $X \sim \mathrm{Ber}(p)$ if
      \[
        X =
        \begin{cases}
          1 & \text{with probability } p, \\
          0 & \text{with probability } q = 1 - p.
        \end{cases}
      \]
      This models a possibly unfair
      coin flip. The Bernoulli random variable $X$
      has pmf
      \[
        p_X(s) =
        \begin{cases}
          p & \text{if } s = 1, \\
          1 - p & \text{if } s = 0, \\
          0 & \text{otherwise}.
        \end{cases}
      \]
    \item \emph{Binomial random variables}:
      For $n \in \N^* = \N \setminus \{0\}$ and $0 < p < 1$, we say that
      $X \sim \mathrm{Bin}(n, p)$ if
      \[
        \PP(X = k) = \binom{n}{k} p^k (1 - p)^{n - k}
      \]
      for $k = 0, 1, \dots, n$ and $\PP(X = k) = 0$
      otherwise. To that this is indeed a pmf,
      observe that
      \[
        \sum_{k = 0}^n \PP(X = k) = \sum_{k = 0}^n \binom{n}{k} p^k (1 - p)^{n - k} = (p + (1 - p))^n = 1^n = 1.
      \]
      The $n = 1$ case reduces to a Bernoulli
      random variable.
    \item \emph{Geometric random variables}:
      For $0 < p < 1$, we say that
      $X \sim \mathrm{Geo}(p)$ if
      \[
        \PP(X = k) = p(1 - p)^{k - 1}
      \]
      for $k = 1, 2, 3, \dots$ and
      $\PP(X = k) = 0$ otherwise. The above function
      is clearly nonnegative and
      \[
        \sum_{k = 1}^\infty p(1 - p)^{k - 1}
        = \frac{p}{1 - (1 - p)} = 1,
      \]
      so this is indeed a pmf. The geometric
      random variable models the
      number of independent Bernoulli trials needed
      to obtain the first success.
  \end{enumerate}
\end{example}

\begin{example}
  Consider the random variable $X$ which counts
  the number of independent Bernoulli trials
  needed to get the 4th success. Note that the
  range of $X$ is $\{4, 5, 6, \dots\}$. Then
  \[
    \PP(X = k) = \binom{k - 1}{3} p^3(1 - p)^{k - 4} p
    = \binom{k - 1}{3} p^4 (1 - p)^{k - 4}
  \]
  for $k = 4, 5, 6, \dots$ and $\PP(X = k) = 0$
  otherwise.
  This is because
  the last trial must be a success and the
  previous $k - 1$ trials need to contain $3$
  successes. Here $X = \mathrm{NBin}(n = 4, p)$,
  the \emph{negative binomial random variable}. In general,
  $X \sim \mathrm{NBin}(n, p)$ takes on values
  $n, n + 1, n + 2, \dots$ and
  \[
    \PP(X = k) = \binom{k - 1}{n - 1} p^n (1 - p)^{k - n}
  \]
  for $k = n, n + 1, n + 2, \dots$. Note that
  the $n = 1$
  case reduces to a geometric random variable.
  The name
  comes from the binomial theorem with negative
  exponents.
\end{example}

\begin{example}
  We say that $X$ is a \emph{Poisson random variable}
  with parameter $\lambda > 0$, written
  $X \sim \mathrm{Poi}(\lambda)$, if $X$ takes the
  values $k = 0, 1, 2, \dots$ with probability mass
  function
  \[
    p_X(k) = \PP(X = k) = \frac{e^{-\lambda} \lambda^k}{k!}.
  \]
  Note that $p_X$ is clearly nonnegative and
  \[
    \sum_{k = 0}^\infty p_X(k) = \sum_{k = 0}^\infty \frac{e^{-\lambda} \lambda^k}{k!}
    = e^{-\lambda} \sum_{k = 0}^\infty \frac{\lambda^k}{k!}
    = e^{-\lambda} e^\lambda = 1,
  \]
  so $p_X$ is indeed a pmf. One can view the
  Poisson random variable in the following manner:
  Suppose $X \sim \mathrm{Bin}(n, p)$ with
  $n \gg 1$ and $p \ll 1$, e.g. $n = 10^5$ and
  $p = 10^{-4}$. Then
  \[
    \PP(X = 100) =
    \binom{10^5}{100} \left(\frac{1}{10^4}\right)^{100}
    \left(1 - \frac{1}{10^4}\right)^{10^5 - 100}.
  \]
  This is very difficult to compute.
  Instead, we approximate this via the Poisson
  random variable.
\end{example}

\begin{prop}
  Let $n \to \infty$ and $p = p(n) \to 0$ in such
  a way that $np(n) \to \lambda > 0$ as $n \to \infty$.
  Then
  \[
    \binom{n}{k} p^k (1 - p)^{n - k} \xrightarrow[n \to \infty]{} \frac{e^{-\lambda} \lambda^k}{k!},
  \]
  i.e. $p_X(k) \to p_Y(k)$ pointwise for
  $k = 0, 1, 2, \dots$, where $X \sim \mathrm{Bin}(n, p)$
  and $Y \sim \mathrm{Poi}(\lambda)$.
\end{prop}

\begin{proof}
  Observe that
  \begin{align*}
    \binom{n}{k} p^k (1 - p)^{n - k}
    &= \frac{n!}{k!(n - k)!} p^k (1 - p)^{n - k}
    = \frac{1}{k!} \left[n(n - 1) \dots (n - k + 1)p^k(1 - p)^{-k} (1 - p)^n\right] \\
    &= \frac{1}{k!} \left[\frac{n(n - 1) \dots (n - k + 1)}{n^k} n^k p^k (1 - p)^{-k} (1 - p)^n\right].
  \end{align*}
  Now notice that $n^k p^k = (np)^k \to \lambda^k$
  since $np \to \lambda$, and
  \[
    \lim_{n \to \infty} \frac{n(n - 1) \dots (n - k + 1)}{n^k} = 1
    \quad \text{and} \quad
    \lim_{p \to 0} (1 - p)^{-k} = 1.
  \]
  Finally, setting $\lambda = np$,
  \[
    (1 - p)^n =
    \left(1 - \frac{\lambda}{n}\right)^n
    \to e^{-\lambda}.
  \]
  Putting all of this together, we see that
  \[
    \binom{n}{k} p^k (1 - p)^{n - k} \xrightarrow[n \to \infty]{} \frac{e^{-\lambda} \lambda^k}{k!},
  \]
  which is the desired result.
\end{proof}

\section{Homework Problems}
Problems \#1, 2, 4, 5, 6, 7, 9, 10 from Grimmett and Welsh.
