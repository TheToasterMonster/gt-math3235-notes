\chapter{Multivariate Discrete Random Variables}

\section{Discrete Random Vectors}
\begin{definition}
  A \emph{random vector} is a function
  from $\Omega$ to $\R^d$, where $d \ge 2$. We say
  that the random vector is
  \emph{bivariate} if $d = 2$,
  \emph{trivariate} if $d = 3$, etc.
\end{definition}

\begin{definition}
  A random vector is said to be \emph{discrete}
  if its range is at most countably infinite.
\end{definition}

\begin{example}
  Let $d = 2$ and $X$ be a $2$-dimensional
  random vector. Then $X : \Omega \to \R^2$ is
  given by
  \[
    \omega \mapsto X(\omega) = (X_1(\omega), X_2(\omega)).
  \]
  In particular, each coordinate $X_1$ and
  $X_2$ is a function from $\Omega$ to $\R$ and
  is thus itself a random variable.
\end{example}

\begin{prop}
  A function $X : \Omega \to \R^d$ with $d \ge 2$
  is a random vector if and only if each of its
  coordinates $X_1, X_2, \dots, X_d$ are random
  variables.
\end{prop}

\begin{proof}
  Most of this is immediate. More justification is
  necessary to ensure measurability (i.e. preimages
  of points are events), but that is
  the subject of a later course in probability.
\end{proof}

\begin{prop}
  A random vector $X : \Omega \to \R^d$ is discrete
  if and only if each of its component random
  variables $X_1, X_2, \dots, X_d$ are discrete.
\end{prop}
\begin{proof}
  ($\Rightarrow$) Observe that there is a
  surjection $\mathcal{R}(X) \to \mathcal{R}(X_i)$
  by projecting onto the $i$th coordinate, so
  $\mathcal{R}(X_i)$ can only be at most countable
  since $\mathcal{R}(X)$ is.

  $(\Leftarrow)$ Notice that
  $\mathcal{R}(X) \subseteq \mathcal{R}(X_1) \times
  \mathcal{R}(X_2) \times \dots \times \mathcal{R}(X_d)$.
  Finite products of countable sets are countable,
  so $\mathcal{R}(X)$ is a subset of a countable
  set and thus countable.
\end{proof}

\begin{definition}
  Let $X : \Omega \to \R^d$ be a discrete random
  vector. The \emph{probability mass function} of $X$
  is
  \[
    p_X(x_1, x_2, \dots, x_d) = \PP(X_1 = x_1, X_2 = x_2, \dots, X_d = x_d)
  \]
  for all $(x_1, x_2, \dots, x_d) \in \mathcal{R}(X)$.
  This probability mass function must satisfy:
  \begin{enumerate}[(i)]
    \item $p_x(x_1, \dots, x_d) \ge 0$,
    \item $\displaystyle \sum_{x_1 \in \mathcal{R}(X_1)} \sum_{x_2 \in \mathcal{R}(X_2)} \dots \sum_{x_d \in \mathcal{R}(X_d)} p_X(x_1, \dots, x_d) = 1$,
    \item and for all $A \subseteq \R^d$,
      \[
        \PP(X \in A)
        = \sum_{x \in A \cap \mathcal{R}(X)}
        p_X(x_1, \dots, x_d).
      \]
      Note that $A \cap \mathcal{R}(X)$ is countable
      even when $A$ might not be.
  \end{enumerate}
\end{definition}

\section{Marginal Distributions}
\begin{definition}
  The \emph{one-dimensional marginal pmf}
  $p_{X_k}$
  of a discrete random vector
  \[X = (X_1, \dots, X_d) : \Omega \to \R^d\]
  with joint pmf $p_X(x_1, \dots, x_d)$
  is given by
  \[
    p_{X_k}(x) = \sum_{x_1 \in \mathcal{R}(X_1)} \dots \sum_{x_{k-1} \in \mathcal{R}(X_{k-1})} \sum_{x_{k+1} \in \mathcal{R}(X_{k+1})} \dots \sum_{x_d \in \mathcal{R}(X_d)} p_X(x_1, \dots, x_{k - 1}, x, x_{k + 1}, \dots x_d).
  \]
  One can similarly define an \emph{$n$-dimensional marginal pmf}
  by summing over all but $n$ of the variables.
  Note that there are a total of $\binom{d}{n}$
  $n$-dimensional marginal pmfs for $X$.
\end{definition}

\begin{remark}
  Note that we indeed have
  \[
    \sum_{x \in \mathcal{R}(X_k)} p_{X_k}(x)
    = \sum_{x_1 \in \mathcal{R}(X_1)} \dots \sum_{x_d \in \mathcal{R}(X_d)} p_X(x_1, \dots, x_d)
    = 1
  \]
  since we can sum in any order by absolute
  convergence. A similar thing works for
  the other marginals.
\end{remark}

\section{Revisiting Expectation}
\begin{definition}
  Let $h : \R^d \to \R$ and $X$ be a $d$-dimensional
  discrete random vector. Then the \emph{expectation} of
  $h(X)$ is
  \[
    \EE[h(X)]
    = \sum_{x_1 \in \mathcal{R}(X_1)} \dots \sum_{x_d \in \mathcal{R}(X_d)} h(x_1, \dots, x_d) p_X(x_1, \dots, x_d),
  \]
  provided that this sum exists and
  converges absolutely. Here $p_X$ is the joint
  pmf of $X$.
\end{definition}

\begin{remark}
  Observe that we have:
  \[
    \begin{tikzcd}
      \Omega \ar[r, "X"] \ar[dr, "h(X)", swap] & \R^d \ar[d, "h"] \\
      & \R
    \end{tikzcd}
  \]
  In particular, we see that $h(X)$ is a random
  variable.
\end{remark}

\begin{prop}
  Let $X_1, X_2$ be two discrete random variables and
  let $a, b \in \R$. Then
  \[
    \EE[aX_1 + bX_2] = a\EE[X_1] + b\EE[X_2],
  \]
  provided the expectations exist.
\end{prop}

\begin{proof}
  Note that $(X_1, X_2)$ is a random vector and
  thus has a joint pmf $p_{(X_1, X_2)}$. Define $h : \R^2 \to \R$ by
  \[
    (x_1, x_2) \mapsto h(x_1, x_2) = ax_1 + bx_2.
  \]
  Then we find that
  \begin{align*}
    \EE[h(X_1, X_2)]
    &= \sum_{x_1 \in \mathcal{R}(X_1)} \sum_{x_2 \in \mathcal{R}(X_2)} h(x_1, x_2) p_{(X_1, X_2)}(x_1, x_2) \\
    &= \sum_{x_1 \in \mathcal{R}(X_1)} \sum_{x_2 \in \mathcal{R}(X_2)} (ax_1 + bx_2) p_{(X_1, X_2)}(x_1, x_2) \\
    &= \sum_{x_1 \in \mathcal{R}(X_1)} \sum_{x_2 \in \mathcal{R}(X_2)} ax_1 p_{(X_1, X_2)}(x_1, x_2)
    + \sum_{x_1 \in \mathcal{R}(X_1)}
    \sum_{x_2 \in \mathcal{R}(X_2)} bx_2 p_{(X_1, X_2)}(x_1, x_2) \\
    &= a \sum_{x_1 \in \mathcal{R}(X_1)} x_1
    \sum_{x_2 \in \mathcal{R}(X_2)} p_{(X_1, X_2)}(x_1, x_2)
    + b \sum_{x_1 \in \mathcal{R}(X_1)} x_2
    \sum_{x_2 \in \mathcal{R}(X_2)} p_{(X_1, X_2)}(x_1, x_2) \\
    &= a \sum_{x_1 \in \mathcal{R}(X_1)} x_1 p_{X_1}(x_1)
    + b \sum_{x_2 \in \mathcal{R}(X_2)} x_2 p_{X_2}(x_2)
    = a \EE[X_1] + b \EE[X_2],
  \end{align*}
  which is the desired result.
  Manipulating the sums above is justified by
  absolute convergence.
\end{proof}

\section{Independence of Random Variables}

\begin{definition}
  Two discrete random variables $X$ and $Y$ are
  said to be \emph{independent}, written
  $X \indep Y$, if for any $x \in \mathcal{R}(X)$
  and $y \in \mathcal{R}(Y)$, the events
  $\{X = x\}$ and $\{Y = y\}$ are independent, i.e.
  \[
    \PP(\{X = x\} \cap \{Y = y\}) = \PP(X = x) \PP(Y = y).
  \]
\end{definition}

\begin{remark}
  We will often write
  $\PP(X = x, Y = y)$ instead of
  $\PP(\{X = x\} \cap \{Y = y\})$.
\end{remark}

\begin{theorem}
  The discrete random variables $X$ and $Y$ are
  independent if and only if there exist functions
  $f, g : \R \to \R$ such that
  \[
    p_{(X, Y)}(x, y) = f(x)g(y)
  \]
  for all $x, y \in \R$. Here $p_{(X, Y)}$ is
  the joint pmf of $(X, Y)$.
\end{theorem}

\begin{proof}
  $(\Leftarrow)$ Assume that
  $p_{(X, Y)}(x, y) = f(x)g(y)$ for all $x, y \in \R$.
  Then
  \[
    p_X(x)
    = \sum_{y \in \mathcal{R}(Y)} p_{(X, Y)}(x, y)
    = \sum_{y \in \mathcal{R}(Y)} f(x) g(y)
    = f(x) \sum_{y \in \mathcal{R}(Y)} g(y),
  \]
  and similarly by symmetry we find that
  \[
    p_Y(x)
    = \sum_{x \in \mathcal{R}(X)} p_{(X, Y)}(x, y)
    = \sum_{x \in \mathcal{R}(X)} f(x) g(y)
    = g(y) \sum_{x \in \mathcal{R}(X)} f(x).
  \]
  But $p_{(X, Y)}$ is the joint pmf, so we must have
  \[
    1 = \sum_{x \in \mathcal{R}(X)} \sum_{y \in \mathcal{R}(Y)} f(x) g(y)
    = \sum_{x \in \mathcal{R}(X)} f(x) \sum_{y \in \mathcal{R}(Y)} g(y).
  \]
  But then we can use this to write
  \begin{align*}
    p_{(X, Y)}(x, y) = f(x) g(y)
    &= f(x) g(y) \sum_{x \in \mathcal{R}(X)} f(x) \sum_{y \in \mathcal{R}(Y)} g(y) \\
    &= \left(f(x) \sum_{y \in \mathcal{R}(Y)} g(y)\right) \left(g(y) \sum_{x \in \mathcal{R}(X)} f(x)\right)
    = p_X(x) p_Y(y),
  \end{align*}
  where the last line follows from the
  previous computations. This is the desired result.

  $(\Leftarrow)$ This is clear. By independence
  $p_{(X, Y)}(x, y) = p_X(x) p_Y(y)$, so we can
  set $f = p_X$ and $g = p_Y$.
\end{proof}

\begin{prop}
  Let $X$ and $Y$ be two independent discrete random
  variables.
  Then $\EE[XY] = \EE[X] \EE[Y]$.
\end{prop}

\begin{proof}
  We can write
  \begin{align*}
    \EE[XY]
    &= \sum_{x \in \mathcal{R}(X)} \sum_{y \in \mathcal{R}(Y)} xy p_{(X, Y)}(x, y)
    = \sum_{x \in \mathcal{R}(X)} \sum_{y \in \mathcal{R}(Y)} xy p_X(x) p_Y(y) \\
    &= \left(\sum_{x \in \mathcal{R}(X)} x p_X(x)\right) \left(\sum_{y \in \mathcal{R}(Y)} y p_Y(y)\right)
    = \EE[X] \EE[Y]
  \end{align*}
  where the second step follows by independence.
\end{proof}

\begin{remark}
  More generally, the same argument shows
  that if $X, Y$ are independent, then
  \[
    \EE[h_1(X)h_2(Y)] = \EE[h_1(X)] \EE[h_2(Y)]
  \]
  for any functions $h_1, h_2 : \R \to \R$.
\end{remark}

\begin{definition}
  Let $X$ and $Y$ be two random variables. Then
  \begin{itemize}
    \item $X, Y$ are
      \emph{uncorrelated} if $\EE[XY] = \EE[X] \EE[Y]$,
      i.e. $\EE[XY] - \EE[X] \EE[Y] = 0$,
    \item $X, Y$ are
      \emph{positively correlated} if $\EE[XY] - \EE[X] \EE[Y] > 0$,
    \item and $X, Y$ are
      \emph{negatively correlated} if $\EE[XY] - \EE[X] \EE[Y] < 0$.
  \end{itemize}
\end{definition}

\begin{remark}
  The previous result shows that if
  $X, Y$ are independent, then they are
  uncorrelated.
\end{remark}

\begin{example}
  However, the converse is not true in general.
  Let $X$ take the values $-1, 0, 1$ with
  probability $1 / 3$. Clearly $\EE[X] = 0$.
  Now set
  \[
    Y = \begin{cases}
      0 & \text{if } X = 0, \\
      1 & \text{if } X \ne 1.
    \end{cases}
  \]
  First observe that $X$ and $Y$ are
  dependent since
  \[
    \PP(X = 0, Y = 1) = 0
    \ne \frac{1}{3} \cdot \frac{2}{3} = \PP(X = 0) \PP(Y = 1).
  \]
  Now since $\EE[X] = 0$, we clearly have
  $\EE[X] \EE[Y] = 0$. Also we can compute that
  \begin{align*}
    \EE[XY]
    &= \sum_{x \in \mathcal{R}(X)} \sum_{y \in \mathcal{R}(Y)} xy \PP(X = x, Y = y) \\
    &= -1 \cdot 1 p_{(X, Y)}(-1, 1) + 1 \cdot 1 p_{(X, Y)}(1, 1)
    = -\frac{1}{3} + \frac{1}{3} = 0.
  \end{align*}
  In particular, this means
  that $\EE[XY] - \EE[X] \EE[Y] = 0 - 0 = 0$,
  so $X$ and $Y$ are uncorrelated. However,
  as we previously computed, $X$ and $Y$ are
  not independent.
\end{example}

\begin{remark}
  If we instead demand that
  $\EE[h_1(X)h_2(Y)] = \EE[h_1(X)] \EE[h_2(Y)]$
  for all functions $h_1, h_2 : \R \to \R$,
  then we are indeed able to conclude that
  $X$ and $Y$ are independent.
\end{remark}

\begin{prop}
  If $\EE[h_1(X)h_2(Y)] = \EE[h_1(X)] \EE[h_2(Y)]$
  for all functions $h_1, h_2 : \R \to \R$, then $X$ and $Y$
  are independent.
\end{prop}

\begin{proof}
  For each $x_1 \in \mathcal{R}(X)$ and $y_1 \in \mathcal{R}(Y)$,
  choose $h_1(x) = \mathbbm{1}_{\{x_1\}}$
  and $h_2(y) = \mathbbm{1}_{\{y_1\}}$. Then
  \[
    \EE[h_1(X)h_2(Y)] = \EE[\mathbbm{1}_{\{x_1\}}(X) \mathbbm{1}_{\{y_1\}}(Y)]
    = \PP(X = x_1, Y = y_1)
  \]
  and also
  \[
    \EE[h_1(X)] \EE[h_2(Y)]
    = \EE[\mathbbm{1}_{\{x_1\}}(X)] \EE[\mathbbm{1}_{\{y_1\}}(Y)]
    = \PP(X = x_1) \PP(Y = y_1),
  \]
  which implies the desired result since
  $\EE[h_1(X)h_2(Y)] = \EE[h_1(X)] \EE[h_2(Y)]$.
\end{proof}

\section{Homework Problems}
Problems \#2, 3, 5, 7, 12, 13, 14 from
Grimmett and Welsh.
