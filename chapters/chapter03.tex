\chapter{Multivariate Discrete Random Variables}

\section{Discrete Random Vectors}
\begin{definition}
  A \emph{random vector} is a function
  from $\Omega$ to $\R^d$, where $d \ge 2$. We say
  that the random vector is
  \emph{bivariate} if $d = 2$,
  \emph{trivariate} if $d = 3$, etc.
\end{definition}

\begin{definition}
  A random vector is said to be \emph{discrete}
  if its range is at most countably infinite.
\end{definition}

\begin{example}
  Let $d = 2$ and $X$ be a $2$-dimensional
  random vector. Then $X : \Omega \to \R^2$ is
  given by
  \[
    \omega \mapsto X(\omega) = (X_1(\omega), X_2(\omega)).
  \]
  In particular, each coordinate $X_1$ and
  $X_2$ is a function from $\Omega$ to $\R$ and
  is thus itself a random variable.
\end{example}

\begin{prop}
  A function $X : \Omega \to \R^d$ with $d \ge 2$
  is a random vector if and only if each of its
  coordinates $X_1, X_2, \dots, X_d$ are random
  variables.
\end{prop}

\begin{proof}
  Most of this is immediate. More justification is
  necessary to ensure measurability (i.e. preimages
  of points are events), but that is
  the subject of a later course in probability.
\end{proof}

\begin{prop}
  A random vector $X : \Omega \to \R^d$ is discrete
  if and only if each of its component random
  variables $X_1, X_2, \dots, X_d$ are discrete.
\end{prop}
\begin{proof}
  ($\Rightarrow$) Observe that there is a
  surjection $\mathcal{R}(X) \to \mathcal{R}(X_i)$
  by projecting onto the $i$th coordinate, so
  $\mathcal{R}(X_i)$ can only be at most countable
  since $\mathcal{R}(X)$ is.

  $(\Leftarrow)$ Notice that
  $\mathcal{R}(X) \subseteq \mathcal{R}(X_1) \times
  \mathcal{R}(X_2) \times \dots \times \mathcal{R}(X_d)$.
  Finite products of countable sets are countable,
  so $\mathcal{R}(X)$ is a subset of a countable
  set and thus countable.
\end{proof}

\begin{definition}
  Let $X : \Omega \to \R^d$ be a discrete random
  vector. The \emph{probability mass function} of $X$
  is
  \[
    p_X(x_1, x_2, \dots, x_d) = \PP(X_1 = x_1, X_2 = x_2, \dots, X_d = x_d)
  \]
  for all $(x_1, x_2, \dots, x_d) \in \mathcal{R}(X)$.
  This probability mass function must satisfy:
  \begin{enumerate}[(i)]
    \item $p_x(x_1, \dots, x_d) \ge 0$,
    \item $\displaystyle \sum_{x_1 \in \mathcal{R}(X_1)} \sum_{x_2 \in \mathcal{R}(X_2)} \dots \sum_{x_d \in \mathcal{R}(X_d)} p_X(x_1, \dots, x_d) = 1$,
    \item and for all $A \subseteq \R^d$,
      \[
        \PP(X \in A)
        = \sum_{x \in A \cap \mathcal{R}(X)}
        p_X(x_1, \dots, x_d).
      \]
      Note that $A \cap \mathcal{R}(X)$ is countable
      even when $A$ might not be.
  \end{enumerate}
\end{definition}

\section{Marginal Distributions}
\begin{definition}
  The \emph{one-dimensional marginal pmf}
  $p_{X_k}$
  of a discrete random vector
  \[X = (X_1, \dots, X_d) : \Omega \to \R^d\]
  with joint pmf $p_X(x_1, \dots, x_d)$
  is given by
  \[
    p_{X_k}(x) = \sum_{x_1 \in \mathcal{R}(X_1)} \dots \sum_{x_{k-1} \in \mathcal{R}(X_{k-1})} \sum_{x_{k+1} \in \mathcal{R}(X_{k+1})} \dots \sum_{x_d \in \mathcal{R}(X_d)} p_X(x_1, \dots, x_{k - 1}, x, x_{k + 1}, \dots x_d).
  \]
  One can similarly define an \emph{$n$-dimensional marginal pmf}
  by summing over all but $n$ of the variables.
  Note that there are a total of $\binom{d}{n}$
  $n$-dimensional marginal pmfs for $X$.
\end{definition}

\begin{remark}
  Note that we indeed have
  \[
    \sum_{x \in \mathcal{R}(X_k)} p_{X_k}(x)
    = \sum_{x_1 \in \mathcal{R}(X_1)} \dots \sum_{x_d \in \mathcal{R}(X_d)} p_X(x_1, \dots, x_d)
    = 1
  \]
  since we can sum in any order by absolute
  convergence. A similar thing works for
  the other marginals.
\end{remark}

\section{Homework Problems}
Problems \#2, 3, 5, 7, 12, 13, 14 from
Grimmett and Welsh.
