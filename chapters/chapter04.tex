\chapter{Probability Generating Functions}

\section{Probability Generating Functions}

The probability generating function is a concept
for discrete random variables, and more precisely
for those $X$ taking values in
$\{0, 1, 2, \dots\}$. Such
random variables have a pmfs $p_X(k) = p_k = \PP(X = k)$
for $k = 0, 1, 2, \dots$. Then
$\{p_k\}_{k \ge 0} \subseteq \R$ is a sequence
such that $p_k \ge 0$ for
each $k$ and $\sum_{k = 0}^\infty p_k = 1$.

\begin{definition}
  Let $X$ be a discrete random variable taking
  values in $\{0, 1, 2, \dots\}$ with pmf given
  by $\PP(X = k) = p_k$ for $k = 0, 1, 2, \dots$.
  Then the \emph{probability generating function (pgf)}
  of $X$ is given by
  \[
    G_X(s) = p_0 + p_1s + p_2s^2 + \dots
    = \EE[s^X],
  \]
  for the values of $s$ for which
  $\EE[|s^X|] = \sum_{k = 0}^\infty p_k |s|^k < \infty$.
\end{definition}

\begin{prop}
  The series $G_X(s) = \sum_{k = 0}^\infty p_k s^k$ converges
  absolutely for $s \in \R$ where $|s| \le 1$.
\end{prop}

\begin{proof}
  Observe that $G_X(s) = \EE[s^X]$
  and for $|s| \le 1$,
  \[
    \EE[|s^X|]
    = \sum_{k = 0}^\infty p_k |s|^k
    \le \sum_{k = 0}^\infty p_k
    = 1
  \]
  since $p_k = p_X(k)$ is a pmf. Thus the
  interval of convergence is guaranteed to contain
  $[-1, 1]$.
\end{proof}

\begin{example}
  Consider the following probability generating
  functions:
  \begin{itemize}
    \item Let $X \sim \mathrm{Ber}(p)$. Then
      \[
        G_X(s) = s^0 \PP(X = 0)
        + s^1 \PP(X = 1)
        = 1(1 - p) + ps = 1 - p + ps.
      \]
      This exists for all $s \in \R$.
    \item Let $X \sim \mathrm{Bin}(n, p)$. Then
      \[
        G_X(s) = \sum_{k = 0}^n s^k \binom{n}{k} p^k (1 - p)^{n - k}
        = \sum_{k = 0}^n \binom{n}{k} (ps)^k (1 - p)^{n - k}
        = (1 - p + ps)^n.
      \]
      This again exists for all $x \in \R$.
    \item Let $X \sim \mathrm{Geo}(p)$. Then
      \[
        G_X(s)
        = \sum_{k = 1}^\infty p(1 - p)^{k - 1} s^k
        = ps \sum_{k = 1}^\infty (1 - p)^{k - 1} s^{k - 1}
        = ps \sum_{k = 0}^\infty ((1 - p) s)^k.
      \]
      When $|(1 - p) s| < 1$, we get
      \[
        G_X(s)
        = \frac{ps}{1 - (1 - p)s}
      \]
      Thus $G_X$ exists
      for $|s| < 1 / (1 - p)$.
    \item Let $X \sim \mathrm{Poi}(\lambda)$.
      Then
      \[
        G_X(s)
        = \sum_{k = 0}^\infty \frac{e^{-\lambda} \lambda^k}{k!} s^k
        = e^{-\lambda} \sum_{k = 0}^\infty \frac{(\lambda s)^k}{k!}
        = e^{-\lambda} e^{\lambda s}
        = e^{\lambda(s - 1)}.
      \]
      This series converges for all $s \in \R$.
  \end{itemize}
\end{example}

\section{Properties of PGFs}
\begin{theorem}
  Let $X$ and $Y$ have probability generating
  functions
  $G_X$ and $G_Y$, respectively.
  Then
  \[G_X(s) = G_Y(s)
    \quad \text{for all $s$ in their interval of convergence} 
  \]
  if and only if $\PP(X = k) = \PP(Y = k)$ for
  all $k = 0, 1, 2, \dots$.
\end{theorem}

\begin{proof}
  $(\Leftarrow)$ If $\PP(X = k) = \PP(Y = k)$ for
  each $k$, then
  \[
    G_X(s)
    = \sum_{k = 0}^\infty \PP(X = k) s^k
    = \sum_{k = 0}^\infty \PP(Y = k) s^k
    = G_Y(s).
  \]
  This holds for every $s$ for which the above series
  converge.

  $(\Rightarrow)$ Assume that
  $G_X(s) = G_Y(s)$ for all
  $|s| \le 1$. First observe that
  plugging in $s = 0$ in $G_X(s)$
  and $G_Y(s)$ immediately
  gives $\PP(X = 0) = \PP(Y = 0)$. Then
  because we have absolute convergence
  for $|s| \le 1$ and thus uniform convergence
  on $|s| \le 1$, we can differentiate
  term by term to get
  \[
    \frac{d}{ds} G_X(s)
    = \frac{d}{ds} \sum_{k = 0}^\infty \PP(X = k) s^k
    = \sum_{k = 0}^\infty \frac{d}{ds} \left[\PP(X = k) s^k\right]
    = \sum_{k = 0}^\infty \PP(X = k) k s^{k - 1}
  \]
  and
  \[
    \frac{d}{ds} G_Y(s)
    = \frac{d}{ds} \sum_{k = 0}^\infty \PP(Y = k) s^k.
    = \sum_{k = 0}^\infty \frac{d}{ds} \left[\PP(Y = k) s^k\right]
    = \sum_{k = 0}^\infty \PP(Y = k) k s^{k - 1}.
  \]
  These are equal since $G_X(s) = G_Y(s)$, so
  we can again evaluate at $s = 0$ to get
  \[
    1 \cdot \PP(X = 1) = 1 \cdot \PP(Y = 1),
  \]
  i.e. $\PP(X = 1) = \PP(Y = 1)$. Continue
  by induction to get $k! \cdot \PP(X = k) = k! \cdot \PP(Y = k)$
  for every $k \ge 1$, so $\PP(X = k) = \PP(Y = k)$
  for each $k$. This is the desired result.
\end{proof}

\begin{theorem}
  Let $X$ be an integer-valued random variable
  with pgf $G_X$. Then for every
  $\ell \ge 1$,
  \[
    \left.\frac{d^\ell}{ds^\ell} G_X(s)\right|_{s = 1}
      = \EE[X(X - 1) \dots (X - \ell + 1)]
  \]
\end{theorem}

\begin{proof}
  As before,
  \[
    \frac{d^\ell}{ds^\ell} G_X(s)
    = \sum_{k = 0}^\infty k(k - 1) \dots (k - \ell + 1)
    \PP(X = k) s^{k - \ell}
  \]
  for all $|s| \le 1$. Evaluating at $s = 1$ gives
  \[
    \frac{d^\ell}{ds^\ell} G_X(s)
    = \sum_{k = \ell}^\infty k(k - 1) \dots (k - \ell + 1) \PP(X = k)
    = \EE[X(X - 1) \dots (X - \ell + 1)],
  \]
  as desired. In particular, this expectation
  is finite if and only if the series on the
  left converges.
\end{proof}

\begin{remark}
  Recall from before that
  $\Var[X] = \EE[X(X - 1)] + \EE[X] - (\EE[X])^2$.
  We can compute this via
  \[
    \Var[X] = G_X''(1) + G_X'(1) - (G_X'(1))^2
  \]
  using the probability generating function if
  $X$ is integer-valued.
\end{remark}

\begin{remark}
  The value $\EE[X(X - 1) \dots (X - \ell + 1)]$
  is called the \emph{factorial moment of order $\ell$} of $X$.
\end{remark}

\begin{theorem}
  Let $X, Y$ be independent random variables.
  Then $G_{X + Y}(s) = G_X(s) G_Y(s)$ for all $|s| \le 1$.
\end{theorem}

\begin{proof}
  We have $G_{X + Y}(s) = \EE[s^{X + Y}] = \EE[s^X s^Y] = \EE[s^X] \EE[s^Y] = G_X(s) G_Y(s)$
  by independence.
\end{proof}

\begin{remark}
  Another way to show this is to use the
  convolution formula for the pmf of $X + Y$.
\end{remark}

\begin{corollary}
  For independent random variables $X_1, \dots, X_n$,
  we have
  \[
    G_{X_1 + \dots + X_n}(s)
    = \prod_{k = 1}^n G_{X_k}(s)
  \]
\end{corollary}

\begin{remark}
  We actually also have the converse:
  If $G_{X + Y}(s) = G_X(s) G_Y(s)$ for all
  $|s| \le 1$, then $X$ and $Y$ are independent.
  One can show this using the convolution
  formula.
\end{remark}

\section{The Random Sum Formula}
Let $S = X_1 + \dots + X_N$, where $X_1, X_2, \dots$
are independent random variables and $N$ is
an integer-valued random variable independent
of the $X_i$. If
$N = 0$, then the sum is empty and by convention
we say that $X_1 + \dots + X_N = 0$. If $N \ge 1$,
then we define
\[
  (X_1 + \dots + X_N)(\omega)
  = X_1(\omega) + X_2(\omega) + \dots + X_{N(\omega)}(\omega).
\]

\begin{theorem}[Random sum formula]
  For integer-valued independent random variables $N, X_1, X_2, \dots$,
  \[
    G_{X_1 + \dots + X_N}(s) = G_N(G_{X}(s)),
  \]
  given that $X_1, X_2, \dots$ are
  identically distributed with $p_X = p_{X_1} = p_{X_2} = \dots$.
\end{theorem}

\begin{proof}
  Since the events $\{N = n\}$ partition
  the sample space $\Omega$,
  by the partition theorem we can write
  \begin{align*}
    G_{X_1 + \dots + X_N}(s)
    = \EE[s^{X_1 + \dots + X_N}]
    &= \sum_{n = 0}^\infty \EE[s^{X_1 + \dots + X_N} | N = n] \PP(N = n) \\
    &= \sum_{n = 0}^\infty \EE[s^{X_1 + \dots + X_n}] \PP(N = n).
  \end{align*}
  Now by independence of the $X_i$, we can
  split the expectation and get
  \[
    G_{X_1 + \dots + X_N}(s)
    = \sum_{n = 0}^\infty G_{X_1}(s) \dots G_{X_n}(s) \PP(N = n)
    = \sum_{n = 0}^\infty (G_X(s)^n) \PP(N = n)
  \]
  since the $X_i$ are identically distributed.
  Now the value on the right is precisely
  $G_N(G_X(s))$.
\end{proof}

\begin{remark}
  We will use ``i.i.d.'' to denote
  ``independent and identically distributed''.
\end{remark}

\begin{remark}
  Set $S = X_1 + \dots + X_N$, so that
  $G_S(s) = G_N(G_X(s))$. Now differentiating
  $G_S(s)$ gives
  \[
    G_S'(s) = G_N'(G_X(s)) G_X'(s).
  \]
  At $s = 1$, we get
  $G_S'(1) = G_N'(G_X(1)) G_X'(1) = G_N'(1) G_X'(1)$.
  But $G_Y'(1) = \EE[Y]$, so this says that
  \[
    \EE[S] = \EE[N] \EE[X]
  \]
  One can continue this process. For instance,
  differentiating again yields
  \[
    G_S''(s)
    = \frac{d}{ds} \left[G_N'(G_X(s)) G_X'(s)\right]
    = G_N''(G_X(s)) G_X'(s) G_X'(s) + G_N'(G_X(s)) G_X''(s).
  \]
  Evaluating at $s = 1$, we obtain
  \begin{align*}
    \EE[S(S - 1)]
    = G_S''(1)
    &= G_N''(1) (G_X'(1))^2 + G_N'(1) G_X''(1) \\
    &= \EE[N(N - 1)] (\EE[X])^2 + \EE[N] \EE[X(X - 1)].
  \end{align*}
  This allows us to find $\Var[S]$ with some more
  work.
\end{remark}

\begin{example}
  Suppose that we have $20$ rabbits, and each
  has probability $1 / 2$ of escaping. The next
  morning, each remaining rabbit gives birth
  to a number of offspring following a Poisson
  distribution with parameter $\lambda = 3$,
  and we want to determine the pgf of the number
  of offspring. To do this, let $N$ be the number
  of rabbits not escaping, and observe that
  $N \sim \mathrm{Bin}(20, 1 / 2)$. Then
  \[
    G_S(s) = G_{X_1 + \dots + X_N}(s)
    = G_N(G_X(s))
  \]
  by the random sum formula, where
  $X \sim \mathrm{Poi}(3)$. Then
  \[
    G_N(s) = \left(\frac{1}{2} + \frac{1}{2}s\right)^{20} \quad \text{and} \quad
    G_X(s) = e^{3(s - 1)},
  \]
  so we find that
  \[
    G_S(s) = G_N(G_X(s))
    = \left(\frac{1}{2} + \frac{1}{2} e^{3(s - 1)}\right)^{20}.
  \]
  In particular, $\EE[S] = \EE[N] \EE[X] = 10(3) = 30$.
\end{example}

\section{Homework Problems}
Problems \#2, 5, 6, 8, 9, 10, 11 from Grimmett and
Welsh.
