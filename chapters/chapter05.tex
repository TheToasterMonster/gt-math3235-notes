\chapter{Continuous Random Variables}

\section{Distribution Functions}
\begin{remark}
  When $X$ is discrete, we have the pmf
  $p_X(x) = \PP(X = x)$ for $x \in \R$. Note that
  $p_X(x) > 0$ for only countably many $x$, and
  outside of $\mathcal{R}(X)$ we ``set $p_X(x) = 0$''
  (really $p_X$ is only defined on $\mathcal{R}(X)$).
  We will now study the situation where we
  have $p_X(x) = 0$ everywhere.
\end{remark}

\begin{definition}
  Let $X$ be a random variable. Then the
  \emph{(cumulative) distribution function (cdf)}
  of $X$ is
  \[
    F_X(x) = \PP(X \le x) \quad \text{for } x \in \R.
  \]
\end{definition}

\begin{remark}
  Note that we can write
  \[
    \PP(X \le x)
    = \PP(\{\omega \in \Omega : X(\omega) \le x\})
    = \PP(\{\omega \in \Omega : X(\omega) \in (-\infty, x]\})
    = \PP(X^{-1}((-\infty, x])).
  \]
  This hints that $X^{-1}((-\infty, x])$ must be in
  the event space $\F$.
\end{remark}

\begin{example}
  Suppose that $X$ is discrete. Then
  \[
    F_X(x) = \sum_{y \in \mathcal{R}(X), y \le x} \PP(X = y).
  \]
  \begin{itemize}
    \item If $X \sim \mathrm{Ber}(p)$, then
      \[
        F_X(x) = \PP(X \le x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          1 - p & \text{if $0 \le x < 1$} \\
          1 & \text{if $x \ge 1$}.
        \end{cases}
      \]
    \item If $X \sim \mathrm{Bin}(n, p)$, then
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          \sum_{\ell = 0}^{k - 1} \binom{n}{\ell} p^\ell (1 - p)^{n - \ell} & \text{if $k - 1 \le x < k$, for $k = 1, 2, \dots, n$,} \\
          1 & \text{if $x \ge n$}.
        \end{cases}
      \]
    \item If $X \sim \Geo(p)$, then
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          1 - (1 - p)^k & \text{if $k \le x < k + 1$, for $k = 0, 1, 2, \dots$}.
        \end{cases}
      \]
      This is because for $k \le x < k + 1$, we have
      \[
        \PP(X \le x)
        = \sum_{\ell = 1}^k p(1 - p)^{\ell - 1}
        = p \sum_{\ell = 0}^{k - 1} p(1 - p)^{\ell}
        = p \frac{1 - (1 - p)^k}{1 - (1 - p)}
        = 1 - (1 - p)^k
      \]
    \item If $X \sim \Poi(\lambda)$, then
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          \sum_{k = 0}^{n} e^{-\lambda} \lambda^k / k! & \text{if $n \le x < n + 1$, for $n = 0, 1, 2, \dots$}.
        \end{cases}
      \]
  \end{itemize}
\end{example}

\begin{prop}
  Let $X$ be a random variable with distribution
  function $F_X$. Then
  \begin{enumerate}[(i)]
    \item $F_X$ is a function $F_X : \R \to [0, 1]$,
    \item $F_X$ is non-decreasing, i.e.
      if $x \le y$, then $F_X(x) \le F_X(y)$,
    \item $\lim_{x \to \infty} F_X(x) = 1$ and
      $\lim_{x \to -\infty} F_X(x) = 0$,
    \item $F_X$ is continuous from the right, i.e.
      $\lim_{y \to x^+} F_X(Y) = F_X(x)$ for every
      $x \in \R$,
    \item $F_X$ has a left-hand limit at
      every $x \in \R$, i.e.
      $F_X(x^-) = \lim_{y \to x^-} F_X(y)$ exists.
    \item $\PP(X = x) = F_X(x) - F_X(x^-)$,
    \item and $\PP(X \in [a, b]) = F_X(b) - F_X(a^-)$.
  \end{enumerate}
\end{prop}

\begin{proof}
  $(i)$ Under $F$, we have $x \mapsto F(x) = \PP(X \le x) \in [0, 1]$.

  $(ii)$ Observe that
  $\{X \le x\} \subseteq \{X \le y\}$ if $x \le y$,
  so $F_X(x) = \PP(X \le x) \le \PP(X \le y) = F_X(y)$.

  $(iii)$ Fix any increasing
  sequence $\{a_n\}$ with $a_n \to \infty$. Then
  $\{X \le a_n\}$ is an increasing sequence of
  events and $\bigcup_{n = 1}^\infty \{X \le a_n\} = \Omega$
  since $a_n \to \infty$. Thus by the continuity
  from below of probability measures,
  \[
    \lim_{n \to \infty} F_X(a_n)
    = \lim_{n \to \infty} \PP(X \le a_n)
    = \PP\left(\bigcup_{n = 1}^\infty \{X \le a_n\}\right)
    = \PP(\Omega)
    = 1
  \]
  Since this holds for any increasing
  sequence $\{a_n\}$, we have
  $\lim_{x \to \infty} F_X(x) = 1$. We immediately
  obtain $\lim_{x \to -\infty} F_X(x) = 0$ as well
  by taking complements in the above argument.

  $(iv)$ Take sequences again. Note that
  $\bigcap_{n = 1}^\infty \{X \le a_n\} = \{X \le y\}$
  if $\{a_n\}$ is decreasing and $a_n \to y$.

  $(v)$ We informally write
  \begin{align*}
    F_X(x^-)
    = \lim_{y \to x^-} F_X(y)
    = \lim_{y \to x^-} \PP(X \le y)
    &= \lim_{y \to x^-} \PP(X \in (-\infty, y]) \\
    &= \PP(X \in (-\infty, x))
    = \PP(X < x).
  \end{align*}
  To justify this, one can take sequences and
  argue as before.

  $(vi)$ This is because
  $F_X(x) - F_X(x^-) = \PP(X \le x) - \PP(X < x) = \PP(X = x)$.

  $(vii)$ Write
  $\PP(X \in [a, b]) = \PP(X \le b) - \PP(X < a) = F_X(b) - F_X(a^-)$.
\end{proof}

\begin{remark}
  Observe that we also have
  \[
    \PP(a < X \le b)
    = \PP(X \in (a, b]))
    = \PP(X \le b) - \PP(X \le a)
    = F_X(b) - F_X(a).
  \]
  We get analogous results for the probability
  of $X$ being in any interval.
\end{remark}

\section{Continuous Random Variables}
\begin{definition}
  A random variable $X$ is
  \emph{continuous} if its distribution function
  $F_X$ is continuous.
\end{definition}

\begin{definition}
  A random variable $X$ is \emph{absolutely continuous}
  if $F_X$ is absolutely continuous, i.e.
  \[
    F_X(x) = \int_{-\infty}^x f_X(t) \, dt
  \]
  for some function $f_X : \R \to \R$ such that
  $f_X(x) \ge 0$ for all $x \in \R$ and
  \[\int_{-\infty}^\infty f_X(x) \, dx = 1\]
  In this case, $f_X$ is called a
  \emph{(probability) density function (pdf)} of $X$.
\end{definition}

\begin{remark}
  Note that density functions of $X$ may differ
  on a set of measure zero, e.g. at a point.
\end{remark}

\begin{remark}
  If $X$ is discrete, then $F_X$ is
  \emph{not} continuous, and thus certainly
  not absolutely continuous.
\end{remark}

\begin{remark}
  If $X$ is absolutely continuous with pdf
  $f_X$, then
  \[
    \PP(X \in (a, b]) = \int_a^b f_X(x) \, dx.
  \]
  In particular, this says that for all $x \in \R$,
  we must have
  \[
    \int_{\{x\}} f_X(t) \, dt = 0.
  \]
  More generally, if $X$ is continuous, then
  $\PP(X = x) = 0$ for every $x \in \R$.
\end{remark}

\section{Common Continuous Random Variables}

\begin{example}
  The following are common continuous random variables:
  \begin{itemize}
    \item \emph{Uniform random variables}: We say that
      $X \sim \Unif([0, 1])$ if
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          x & \text{if $0 \le x < 1$} \\
          1 & \text{if $x \ge 1$}.
        \end{cases}
      \]
      In general, for $-\infty < a < b < \infty$, we
      say that $X \sim \Unif([a, b])$ if
      $(X - a) / (b - a) \sim \Unif([0, 1])$. In
      this case, $X$ has pdf
      \[
        f_X(x) =
        \begin{cases}
          1 / (b - a) & \text{if $a \le x \le b$} \\
          0 & \text{otherwise}.
        \end{cases}
      \]
      Note that $f_X$ is continuous and
      also differentiable everywhere except at
      $x = a$ and $x = b$.
    \item \emph{Exponential random variables}: For
      $\lambda > 0$, we say that $X \sim \Exp(\lambda)$
      if $X$ has density function
      \[
        f_X(t) =
        \begin{cases}
          \lambda e^{-\lambda t} & \text{if $t > 0$} \\
          0 & \text{otherwise}.
        \end{cases}
      \]
      The cdf of $X$ is
      \[
        F_X(x) = \int_{-\infty}^x f_X(t) \, dt =
        \begin{cases}
          0 & \text{if $x \le 0$} \\
          1 - e^{-\lambda x} & \text{if $x > 0$}.
        \end{cases}
      \]
      Note that $F_X$ is continuous and also
      differentiable everywhere except $x = 0$.
  \end{itemize}
\end{example}

\begin{remark}
  The \emph{fundamental theorem of calculus} says that
  for a fixed $c \in \R$,
  \[
    \frac{d}{dx} \int_{c}^x f(t) \, dt = f(x).
  \]
  When applied to distribution functions, this says
  that
  \[
    \frac{d}{dx} F_X(x) =
    \frac{d}{dx} \int_{-\infty}^x f_X(t) \, dt
    = f_X(x),
  \]
  for almost all $x \in \R$ (whenever $F_X$ is
  differentiable at $x$).
\end{remark}

\section{Homework Problems}
Problems \#1, 2, 3, 4, 7, 9, 10, 14 from Grimmett and Welsh.
