\chapter{Continuous Random Variables}

\section{Distribution Functions}
\begin{remark}
  When $X$ is discrete, we have the pmf
  $p_X(x) = \PP(X = x)$ for $x \in \R$. Note that
  $p_X(x) > 0$ for only countably many $x$, and
  outside of $\mathcal{R}(X)$ we ``set $p_X(x) = 0$''
  (really $p_X$ is only defined on $\mathcal{R}(X)$).
  We will now study the situation where we
  have $p_X(x) = 0$ everywhere.
\end{remark}

\begin{definition}
  Let $X$ be a random variable. Then the
  \emph{(cumulative) distribution function (cdf)}
  of $X$ is
  \[
    F_X(x) = \PP(X \le x) \quad \text{for } x \in \R.
  \]
\end{definition}

\begin{remark}
  Note that we can write
  \[
    \PP(X \le x)
    = \PP(\{\omega \in \Omega : X(\omega) \le x\})
    = \PP(\{\omega \in \Omega : X(\omega) \in (-\infty, x]\})
    = \PP(X^{-1}((-\infty, x])).
  \]
  This hints that $X^{-1}((-\infty, x])$ must be in
  the event space $\F$.
\end{remark}

\begin{example}
  Suppose that $X$ is discrete. Then
  \[
    F_X(x) = \sum_{y \in \mathcal{R}(X), y \le x} \PP(X = y).
  \]
  \begin{itemize}
    \item If $X \sim \mathrm{Ber}(p)$, then
      \[
        F_X(x) = \PP(X \le x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          1 - p & \text{if $0 \le x < 1$} \\
          1 & \text{if $x \ge 1$}.
        \end{cases}
      \]
    \item If $X \sim \mathrm{Bin}(n, p)$, then
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          \sum_{\ell = 0}^{k - 1} \binom{n}{\ell} p^\ell (1 - p)^{n - \ell} & \text{if $k - 1 \le x < k$, for $k = 1, 2, \dots, n$,} \\
          1 & \text{if $x \ge n$}.
        \end{cases}
      \]
    \item If $X \sim \Geo(p)$, then
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          1 - (1 - p)^k & \text{if $k \le x < k + 1$, for $k = 0, 1, 2, \dots$}.
        \end{cases}
      \]
      This is because for $k \le x < k + 1$, we have
      \[
        \PP(X \le x)
        = \sum_{\ell = 1}^k p(1 - p)^{\ell - 1}
        = p \sum_{\ell = 0}^{k - 1} p(1 - p)^{\ell}
        = p \frac{1 - (1 - p)^k}{1 - (1 - p)}
        = 1 - (1 - p)^k
      \]
    \item If $X \sim \Poi(\lambda)$, then
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          \sum_{k = 0}^{n} e^{-\lambda} \lambda^k / k! & \text{if $n \le x < n + 1$, for $n = 0, 1, 2, \dots$}.
        \end{cases}
      \]
  \end{itemize}
\end{example}

\begin{prop}
  Let $X$ be a random variable with distribution
  function $F_X$. Then
  \begin{enumerate}[(i)]
    \item $F_X$ is a function $F_X : \R \to [0, 1]$,
    \item $F_X$ is non-decreasing, i.e.
      if $x \le y$, then $F_X(x) \le F_X(y)$,
    \item $\lim_{x \to \infty} F_X(x) = 1$ and
      $\lim_{x \to -\infty} F_X(x) = 0$,
    \item $F_X$ is continuous from the right, i.e.
      $\lim_{y \to x^+} F_X(Y) = F_X(x)$ for every
      $x \in \R$,
    \item $F_X$ has a left-hand limit at
      every $x \in \R$, i.e.
      $F_X(x^-) = \lim_{y \to x^-} F_X(y)$ exists.
    \item $\PP(X = x) = F_X(x) - F_X(x^-)$,
    \item and $\PP(X \in [a, b]) = F_X(b) - F_X(a^-)$.
  \end{enumerate}
\end{prop}

\begin{proof}
  $(i)$ Under $F$, we have $x \mapsto F(x) = \PP(X \le x) \in [0, 1]$.

  $(ii)$ Observe that
  $\{X \le x\} \subseteq \{X \le y\}$ if $x \le y$,
  so $F_X(x) = \PP(X \le x) \le \PP(X \le y) = F_X(y)$.

  $(iii)$ Fix any increasing
  sequence $\{a_n\}$ with $a_n \to \infty$. Then
  $\{X \le a_n\}$ is an increasing sequence of
  events and $\bigcup_{n = 1}^\infty \{X \le a_n\} = \Omega$
  since $a_n \to \infty$. Thus by the continuity
  from below of probability measures,
  \[
    \lim_{n \to \infty} F_X(a_n)
    = \lim_{n \to \infty} \PP(X \le a_n)
    = \PP\left(\bigcup_{n = 1}^\infty \{X \le a_n\}\right)
    = \PP(\Omega)
    = 1
  \]
  Since this holds for any increasing
  sequence $\{a_n\}$, we have
  $\lim_{x \to \infty} F_X(x) = 1$. We immediately
  obtain $\lim_{x \to -\infty} F_X(x) = 0$ as well
  by taking complements in the above argument.

  $(iv)$ Take sequences again. Note that
  $\bigcap_{n = 1}^\infty \{X \le a_n\} = \{X \le y\}$
  if $\{a_n\}$ is decreasing and $a_n \to y$.

  $(v)$ We informally write
  \begin{align*}
    F_X(x^-)
    = \lim_{y \to x^-} F_X(y)
    = \lim_{y \to x^-} \PP(X \le y)
    &= \lim_{y \to x^-} \PP(X \in (-\infty, y]) \\
    &= \PP(X \in (-\infty, x))
    = \PP(X < x).
  \end{align*}
  To justify this, one can take sequences and
  argue as before.

  $(vi)$ This is because
  $F_X(x) - F_X(x^-) = \PP(X \le x) - \PP(X < x) = \PP(X = x)$.

  $(vii)$ Write
  $\PP(X \in [a, b]) = \PP(X \le b) - \PP(X < a) = F_X(b) - F_X(a^-)$.
\end{proof}

\begin{remark}
  Observe that we also have
  \[
    \PP(a < X \le b)
    = \PP(X \in (a, b]))
    = \PP(X \le b) - \PP(X \le a)
    = F_X(b) - F_X(a).
  \]
  We get analogous results for the probability
  of $X$ being in any interval.
\end{remark}

\section{Continuous Random Variables}
\begin{definition}
  A random variable $X$ is
  \emph{continuous} if its distribution function
  $F_X$ is continuous.
\end{definition}

\begin{definition}
  A random variable $X$ is \emph{absolutely continuous}
  if $F_X$ is absolutely continuous, i.e.
  \[
    F_X(x) = \int_{-\infty}^x f_X(t) \, dt
  \]
  for some function $f_X : \R \to \R$ such that
  $f_X(x) \ge 0$ for all $x \in \R$ and
  \[\int_{-\infty}^\infty f_X(x) \, dx = 1\]
  In this case, $f_X$ is called a
  \emph{(probability) density function (pdf)} of $X$.
\end{definition}

\begin{remark}
  Note that density functions of $X$ may differ
  on a set of measure zero, e.g. at a point.
\end{remark}

\begin{remark}
  If $X$ is discrete, then $F_X$ is
  \emph{not} continuous, and thus certainly
  not absolutely continuous.
\end{remark}

\begin{remark}
  If $X$ is absolutely continuous with pdf
  $f_X$, then
  \[
    \PP(X \in (a, b]) = \int_a^b f_X(x) \, dx.
  \]
  In particular, this says that for all $x \in \R$,
  we must have
  \[
    \int_{\{x\}} f_X(t) \, dt = 0.
  \]
  More generally, if $X$ is continuous, then
  $\PP(X = x) = 0$ for every $x \in \R$.
\end{remark}

\section{Common Continuous Random Variables}

\begin{example}
  The following are common continuous random variables:
  \begin{itemize}
    \item \emph{Uniform random variables}: We say that
      $X \sim \Unif([0, 1])$ if
      \[
        F_X(x) =
        \begin{cases}
          0 & \text{if $x < 0$} \\
          x & \text{if $0 \le x < 1$} \\
          1 & \text{if $x \ge 1$}.
        \end{cases}
      \]
      In general, for $-\infty < a < b < \infty$, we
      say that $X \sim \Unif([a, b])$ if
      $(X - a) / (b - a) \sim \Unif([0, 1])$. In
      this case, $X$ has pdf
      \[
        f_X(x) =
        \begin{cases}
          1 / (b - a) & \text{if $a \le x \le b$} \\
          0 & \text{otherwise}.
        \end{cases}
      \]
      Note that $f_X$ is continuous and
      also differentiable everywhere except at
      $x = a$ and $x = b$.
    \item \emph{Exponential random variables}: For
      $\lambda > 0$, we say that $X \sim \Exp(\lambda)$
      if $X$ has density function
      \[
        f_X(t) =
        \begin{cases}
          \lambda e^{-\lambda t} & \text{if $t > 0$} \\
          0 & \text{otherwise}.
        \end{cases}
      \]
      The cdf of $X$ is
      \[
        F_X(x) = \int_{-\infty}^x f_X(t) \, dt =
        \begin{cases}
          0 & \text{if $x \le 0$} \\
          1 - e^{-\lambda x} & \text{if $x > 0$}.
        \end{cases}
      \]
      Note that $F_X$ is continuous and also
      differentiable everywhere except $x = 0$.
  \end{itemize}
\end{example}

\begin{remark}
  The \emph{fundamental theorem of calculus} says that
  for a fixed $c \in \R$,
  \[
    \frac{d}{dx} \int_{c}^x f(t) \, dt = f(x).
  \]
  When applied to distribution functions, this says
  that
  \[
    \frac{d}{dx} F_X(x) =
    \frac{d}{dx} \int_{-\infty}^x f_X(t) \, dt
    = f_X(x),
  \]
  for almost all $x \in \R$ (whenever $F_X$ is
  differentiable at $x$).
\end{remark}

\begin{example}
  A random variable $X$ is said to be \emph{normal}
  (or \emph{Gaussian}) with parameters $\mu \in \R$
  and $\sigma^2$ such that $\sigma > 0$, written
  $X \sim \mathcal{N}(\mu, \sigma^2)$,
  if its density is given by
  \[
    f_X(x) = \frac{1}{\sqrt{2\pi \sigma^2}} e^{-(x - \mu)^2 / (2\sigma^2)}
  \]
  for $-\infty < x < \infty$.
  When $\mu = 0$, $\sigma^2 = 1$, then $X$ is
  said to be the \emph{standard normal} distribution,
  i.e.
  \[
    f_X(x) = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}.
  \]
  To verify that this is a density,
  first note that clearly $f_X(x) \ge 0$.
  Now set $\displaystyle I = \int_{-\infty}^\infty e^{-x^2 / 2} \, dx$.
  Then
  \[
    I^2 = \int_{-\infty}^\infty e^{-x^2 / 2} \, dx \int_{-\infty}^\infty e^{-y^2 / 2} \, dy
    = \int_{-\infty}^\infty \int_{-\infty}^\infty e^{-(x^2 + y^2) / 2} \, dx \, dy.
  \]
  This step is justified by the Fubini-Tonelli theorem
  since $e^{-(x^2 + y^2) / 2} \ge 0$. Then change
  to polar coordinates (with $(x, y) = (r\cos \theta, r \sin \theta)$ and $dx dy = r\, dr d\theta$)
  to get
  \[
    I^2 =
    \int_0^\infty
    \int_0^{2\pi}
    r e^{-r^2 / 2}
    \, dr \, d\theta
    = 2\pi \int_0^\infty r e^{-r^2 / 2} \, dr.
    = 2\pi \left[ e^{-r^2 / 2} \right]_{r = 0}^{r = \infty}
    = 2\pi.
  \]
  Thus $I = \sqrt{2\pi}$ (since $I \ge 0$), so
  $\displaystyle \int_{-\infty}^\infty f_X(x) \, dx = I / \sqrt{2\pi} = 1$. Thus $f_X$ is a density.
  The fact that the general pdf $f_X$ is a density
  follows from a change of coordinates
  $y = (x - \mu) / \sigma$ with $dx = \sigma\, dy$,
  so that
  \[
    \frac{1}{\sqrt{2\pi \sigma^2}}
    \int_{-\infty}^\infty e^{-(x - \mu)^2 / (2\sigma^2)} \, dx
    = \frac{1}{\sqrt{2\pi \sigma^2}} \int_{-\infty}^\infty
    e^{-y^2 / 2} \sigma\, dy = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-y^2 / 2} \, dy = 1,
  \]
  where the latter integral is what we
  computed previously.
\end{example}

\section{Expectation of Continuous Random Variables}
\begin{theorem}[Law of the subconscious statistician]
  Let $X$ be a random variable with pdf $f_X$, and let
  $h : \R \to \R$ be a sufficiently nice function.\footnote{Here $h$ must be \emph{measurable}.}
  Let $Y = h(X)$. Then
  \[
    \EE[Y] = \EE[h(X)] = \int_{-\infty}^\infty h(x) f_X(x) \, dx,
  \]
  provided that
  $\displaystyle \int_{-\infty}^\infty h(x) f_X(x) \, dx$ converges
  absolutely, i.e.
  $\displaystyle \int_{-\infty}^\infty |h(x)| f_X(x) \, dx < \infty$.\footnote{The integral of $h(x) F_X(x)$ must be absolutely convergent for its \emph{Lebesgue integral} to exist.}
\end{theorem}

\begin{proof}
  Use the Riemann sum approximation. First
  restrict to a compact interval, e.g. $[0, 1]$.
  Then
  \[
    \int_0^1 h(x) f_X(x) \, dx
    \approx \sum_{i = 1}^n h(x_i) f_X(x_i) \Delta x
  \]
  for large $n$. Now appeal to the discrete
  version of this result.
\end{proof}

\begin{example}
  Let $X \sim \mathcal{N}(0, 1)$. Then we would like
  to find
  \[
    \EE[X] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty x e^{-x^2 / 2} \, dx.
  \]
  First we must check that this integral
  converges absolutely. We write
  \[
    \EE[|X|]
    = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty |x| e^{-x^2 / 2}\, dx
    = \lim_{M_1, M_2 \to \infty}
    \frac{1}{\sqrt{2\pi}} \int_{-M_1}^{M_2} |x| e^{-x^2 / 2} \, dx.
  \]
  In the finite setting, we can split the integral to
  get
  \begin{align*}
    \EE[|X|]
    &= \lim_{M_1, M_2 \to \infty}
    \left[
      \frac{1}{\sqrt{2\pi}} \int_0^{M_2} x e^{-x^2 / 2} \, dx
      - \frac{1}{\sqrt{2\pi}} \int_{-M_1}^0 x e^{-x^2 / 2} \, dx
    \right] \\
    &= \lim_{M_2 \to \infty} \left[\frac{1}{\sqrt{2\pi}} (-e^{-x^2 / 2})\right]_{x = 0}^{x = M_2}
    + \lim_{M_1 \to \infty} \left[\frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}\right]_{x = -M_1}^{x = 0} \\
    &= \lim_{M_2 \to \infty} \frac{1}{\sqrt{2\pi}} (1 - e^{-M_2^2 / 2})
    + \lim_{M_1 \to \infty} \frac{1}{\sqrt{2\pi}} (1 - e^{-M_1^2 / 2})
    = \sqrt{2 / \pi}.
  \end{align*}
  This establishes that $\EE[X]$ converges
  absolutely. Thus we can argue that
  \[
    \EE[X] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty x e^{-x^2 / 2} \, dx = 0
  \]
  since $x e^{-x^2 / 2}$ is an odd function on $\R$
  (we can restrict to $[-M, M]$
  and then take $M \to \infty$).
\end{example}

\begin{example}
  Now let $X \sim \mathcal{N}(\mu, \sigma^2)$, and
  we find $\EE[X]$. We have
  \[
    \EE[X] = \frac{1}{\sqrt{2\pi \sigma^2}} \int_{-\infty}^\infty x e^{-(x - \mu)^2 / (2\sigma^2)} \, dx.
  \]
  Make the change of variables $y = (x - \mu) / \sigma$
  with $dx = \sigma\, dy$ to get
  \begin{align*}
    \EE[X]
    = \frac{1}{\sqrt{2\pi \sigma^2}}
    \int_{-\infty}^\infty (\sigma y + \mu) e^{-y^2 / 2} \sigma\, dy
    &= \frac{\sigma}{\sqrt{2\pi}} \int_{-\infty}^\infty y e^{-y^2 / 2} \, dy + \frac{\mu}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-y^2 / 2} \, dy \\
    &= 0 + \mu = \mu
  \end{align*}
\end{example}

\begin{example}
  Let $X \sim \mathcal{N}(0, 1)$, and we find
  $\EE[X^2]$. We have
  \[
    \EE[X^2] = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty x^2 e^{-x^2 / 2} \, dx.
  \]
  This integrand is nonnegative, so no need to justify
  absolute convergence. Now integrate by parts
  with $u(x) = x$ and $v'(x) = x e^{-x^2 / 2}$ (with
  $u'(x) = 1$ and $v(x) = -e^{-x^2 / 2}$) to get
  \[
    \EE[X^2] = \frac{1}{\sqrt{2\pi}} \left[ -x e^{-x^2 / 2} \right]_{x = -\infty}^{x = \infty} + \frac{1}{\sqrt{2\pi}} \int_{-\infty}^\infty e^{-x^2 / 2} \, dx = 0 + 1 = 1.
  \]
  In particular, this gives
  $\Var[X] = \EE[X^2] - (\EE[X])^2 = 1 - 0 = 1$.
\end{example}

\begin{example}
  In general, for $X \sim \mathcal{N}(\mu, \sigma^2)$,
  we can compute
  \[
    \Var[X] = \EE[(X - \EE[X])^2]
    = \EE[(X - \mu)^2]
    = \frac{1}{\sqrt{2\pi \sigma^2}}
    \int_{-\infty}^\infty (x - \mu)^2 e^{-(x - \mu)^2 / 2\sigma^2}\, dx.
  \]
  Now change variables with
  $y = (x - \mu) / \sigma$ and $dx = \sigma\, dy$ to get
  \[
    \Var[X] = \frac{1}{2\pi \sigma^2} \int_{-\infty}^\infty \sigma^2 y^2 e^{-y^2 / 2} \sigma\, dy
    = \frac{\sigma^2}{\sqrt{2\pi}} \int_{-\infty}^\infty y^2 e^{-y^2 / 2} \, dy
    = \sigma^2.
  \]
\end{example}

\begin{remark}
  The previous computations show that
  for $X \sim \mathcal{N}(\mu, \sigma^2)$, we have
  $\EE[X] = \mu$ and $\Var[X] = \sigma^2$.
\end{remark}

\section{Geometric Probability}

\begin{example}[Bertrand's paradox]
  Consider a unit disk $U$, and choose a chord
  $C$ of the disk at random. Let $T$ be the
  equilateral triangle with one side $C$ (there
  are two, choose the one that points towards the
  center of the disk). Then what is $\PP(T \subseteq U)$?
  The issue comes from how we choose $C$:
  \begin{enumerate}
    \item Suppose we choose angles $X, Y \in [0, 2\pi]$
      uniformly at random, and then choose the chord
      $C$ to go through $X, Y$. This amounts to
      choose the difference uniformly at random
      in $[0, 2\pi]$, since the problem is
      rotationally invariant. We need $|X - Y| \le 2\pi / 3$, so
      \[\PP(T \subseteq U) = \PP(-2\pi / 3 \le X - Y \le 2 \pi / 3) = \frac{2}{3}\]
    \item Alternatively we could choose a point
      $X$ at random and then choose a random angle
      $\theta \in [0, \pi / 2]$ (from the tangent
      line to $U$ at $X$). Clearly $X$ does not
      matter, and we need $\theta \le \pi / 3$.
      This gives
      \[\PP(T \subseteq U) =
        \PP(\theta \le \pi / 3) = 1 / 3
        = \frac{\pi / 3}{\pi / 2} = \frac{2}{3}.
      \]
    \item We could also choose a direction
      $\theta$, which determines a line through the
      circle. Then choose a point $M \in [-1, 1]$
      uniformly at random on this line, which
      determines a chord if we let $M$ be the
      midpoint and say that the chord is perpendicular
      to the line. Then we need
      $- 1 / 2 \le M \le 1 / 2$, os
      \[
        \PP(T \subseteq U) = \PP(-1 / 2 \le M \le 1 / 2) =
        \frac{1}{2}.
      \]
    \item Finally, we could simply choose the
      midpoint $M \in U$ at random. We need
      $M$ to be within the circle of radius $1 / 2$
      with the same center as $M$. Thus
      in this case
      \[
        \PP(T \subseteq U) = \frac{\pi - \pi / 4}{\pi}
        = \frac{3}{4}.
      \]
  \end{enumerate}
  We see that, depending on the method in
  which we choose the chord, the answer may be
  different.
\end{example}

\begin{example}[Buffon's needle]
  Consider horizontal lines in the plane, each
  spaced distance $1$ apart. Drop a needle of
  length $1$ at random. What is the probability that
  the needle intersects a line?
  \begin{itemize}
    \item One way to proceed is to choose
      $(X, Y)$ first as the position of the center
      of the needle. Then choose $\theta \in [0, \pi]$
      to be the angle of the needle. Note the
      actual position of the needle does not matter,
      so we can simply choose $Y \in [0, 1]$. Now
      there are two cases:
      \begin{enumerate}
        \item If the needle intersects with the upper
          line, then we need
          \[
            Y - \frac{1}{2} \sin \theta \le 1 \le Y + \frac{1}{2} \sin \theta \iff 1 - \frac{1}{2} \sin \theta \le Y \le 1 + \frac{1}{2} \sin \theta.
          \]
          Note that the second inequality is always
          true, so we only need $Y \ge 1 - \frac{1}{2} \sin \theta$.
        \item If the needle intersects with the lower
          line, then we need
          \[
            Y - \frac{1}{2} \sin \theta \le 0 \le Y + \frac{1}{2} \sin \theta \iff -\frac{1}{2} \sin \theta \le Y \le \frac{1}{2} \sin \theta.
        \]
        This time the first inequality is always
        true, so we just need $Y \le \frac{1}{2} \sin \theta$.
      \end{enumerate}
      So for the needle to intersect the
      lines, we need $Y \le \frac{1}{2} \sin \theta$
      or $Y \ge 1 - \frac{1}{2} \sin \theta$.
      Consider the subset of $[0, \pi] \times [0, 1]$
      which satsifies the above condition. There
      are two symmetric regions of area
      \[
        \int_0^\pi \frac{1}{2} \sin \theta\, d\theta
        = \frac{1}{2} [-\cos \theta]_{\theta = 0}^{\theta = \pi}
        = \frac{1}{2} [1 - (-1)] = 1.
      \]
      Thus the area is $2$, and the probability is
      $2 / \pi$.
  \end{itemize}
  This gives one way to estimate hte value of $\pi$.
  Let $I_n$ be the number of times the needle
  intersects with a line in $n$ trials. Then
  $I_n / n \to 2 / \pi$ as $n \to \infty$, so
  \[
    \widehat{\pi}_n = \frac{2n}{I_n} \to \pi.
  \]
  This is called the \emph{Monte Carlo method},
  which can also be used to estimate integrals.\footnote{Note that there are much more efficient methods than Monte Carlo for estimating $\pi$.}
\end{example}

\begin{example}[Stick breaking]
  Choose $X, Y \in [0, 1]$ uniformly at random.
  This determines two cuts of $[0, 1]$, which
  breaks the interval into three segments. What is
  the probability that these segments form the
  sides of a triangle?
  \begin{itemize}
    \item
  The pieces are of length $X, Y - X, 1 - Y$
  (assume that $X \le Y$, which we can do since
  we are choosing uniformly at random).
      By the triangle inequality, we need
      \[
        \begin{cases}
          X + (Y - X) \ge 1 - Y \\
          (Y - X) + (1 - Y) > X \\
          X + (1 - Y) > Y - X
        \end{cases}
        \implies \quad
        \begin{cases}
          Y > 1 / 2 \\
          X < 1 / 2 \\
          Y < 1 / 2 + X.
        \end{cases}
      \]
      Draw the region in $[0, 1] \times [0, 1]$ that
      satisfies the above, and we find that the probability
      is $1 / 4$.
  \end{itemize}
\end{example}

\begin{example}
  Choose $n$ points on a circle uniformly
  at random. What is the probability that
  all $n$ points lie in some semicircle?
  \begin{itemize}
    \item Let $A$ be the event
      $\{\text{all $n$ points lie in some semicircle}\}$.
      The goal is to work with easier events.
      Number the points by $P_1, P_2, \dots, P_n$.
      Then let $A_i$ be the event that
      $P_1, P_2, \dots, P_n$ lie in the semicircle
      which begins at $P_i$. Then we easily find that
      $\PP(A_i) = 1 / 2^{n - 1}$ for each $1 \le i \le n$.
      Also
      \[
        A = A_1 \cup A_2 \cup \dots A_n,
      \]
      where the union is disjoint, since we cannot
      have two different points both be the first point
      in a semicircle (note that $\PP(P_i = P_j) = 0$
      for $i \ne j$). Thus
      \[
        \PP(A) = \PP(A_1 \cup \dots \cup A_n)
        = \PP(A_1) + \dots + \PP(A_n)
        = n \PP(A_1) = \frac{n}{2^{n - 1}}
      \]
      since the union is disjoint.
  \end{itemize}
  If the problem is more difficult (the union is
  not disjoint), one can try to apply
  inclusion-exclusion.
\end{example}

\section{Homework Problems}
Problems \#1, 2, 3, 4, 7, 9, 10, 14 from Grimmett and Welsh.
