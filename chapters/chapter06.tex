\chapter{Multivariate Continuous Distributions}

\section{Multivariate Absolutely Continuous Distributions}
\begin{definition}
  Let $(X, Y)$ be a two-dimensional random vector,
  its \emph{distribution function} is given by
  \[
    F_{(X, Y)}(x, y) = \PP(X \leq x, Y \leq y)
    \quad \text{for $(x, y) \in \R^2$}.
  \]
\end{definition}

\begin{prop}
  We have the following properties for the
  distribution function of $(X, Y)$:
  \begin{enumerate}[(i)]
    \item $\lim_{x \to -\infty} F_{(X, Y)}(x, y) = 0$
      and $\lim_{y \to -\infty} F_{(X, Y)}(x, y) = 0$.
    \item $\lim_{x, y \to \infty} F_{(X, Y)}(x, y) = 1$.
    \item $F_{(X, Y)}$ is weakly increasing
      in each variable: If $x_1 \le x_2$ then
      $F_{(X, Y)}(x_1, y) \le F_{(X, Y)}(x_2, y)$
      and if $y_1 \le y_2$ then
      $F_{(X, Y)}(x, y_1) \le F_{(X, Y)}(x, y_2)$.
    \item If $x_1 \le y_1$ and $x_2 \le y_2$, then\footnote{One can interpret this integral as a \emph{Riemann-Stieltjes integral}.}
      \[
        \int_{x_1}^{x_2} \int_{y_1}^{y_2} dF(u, v)
        \ge 0.
      \]
  \end{enumerate}
\end{prop}

\begin{proof}
  $(i)$ We have
  \begin{align*}
    \lim_{x \to -\infty} F_{(X, Y)}(x, y)
    &= \lim_{x \to -\infty} \PP(X^{-1}((-\infty, x])
    \cap Y^{-1}((-\infty, y])) \\
    &= \PP(Y^{-1}((-\infty, y]) \cap \bigcap_{x \in \R} X^{-1}((-\infty, x]) \\
    &= \PP(Y^{-1}((-\infty, y]) \cap \varnothing)
    = 0.
  \end{align*}
  $(ii)$ We have
  \[
    \lim_{x \to \infty} F_{(X, Y)}(x, y)
    = \PP(\{X \le \infty\} \cap \{Y \le y\})
    = \PP(Y \le y) = F_Y(y).
  \]
  Then taking $y \to \infty$ gives
  $\lim_{y \to \infty} F_Y(y) = 1$.

  $(iii)$ If $x_1 \le x_2$ and $y$ is fixed, then
  $\PP(X \le x_1, Y \le y) \le \PP(X \le x_2, Y \le y)$ since
  \[
    X^{-1}((-\infty, x_1]) \cap Y^{-1}((-\infty, y])
    \subseteq X^{-1}((-\infty, x_2]) \cap Y^{-1}((-\infty, y]).
  \]

  $(iv)$ The integral corresponds to $\PP(X \in (x_1, x_2), Y \in (y_1, y_2))$,
  which must be nonnegative.
\end{proof}

\begin{remark}
  The above properties characterize distribution
  functions of two-dimensional random vectors.
\end{remark}

\section{Independence of Continuous Random Variables}
\begin{definition}
  Two random variables $X, Y$ are said to be
  \emph{independent}
  if for all $x, y \in \R$, the events
  $\{X \le x\}$ and $\{Y \le y\}$ are independent.
\end{definition}

\begin{prop}
  Two continuous random variables $X, Y$ are independent
  if and only if \[
    F_{(X, Y)}(x, y) = F_X(x) F_Y(y)
  \]
  for every $x, y \in \R$.
\end{prop}

\begin{proof}
  $(\Rightarrow)$ If $X$ and $Y$ are independent,
  then $\{X \le x\}$ and $\{Y \le y\}$ are independent
  for all $x, y \in \R$, so
  \[
    \PP(\{X \le x\} \cap \{Y \le y\}) = \PP(\{X \le x\}) \PP(\{Y \le y\}),
  \]
  i.e. we have $F_{(X, Y)}(x, y) = F_X(x) F_Y(y)$.

  $(\Leftarrow)$ If $F_{(X, Y)}(x, y) = F_X(x) F_Y(y)$,
  then for all $x, y \in \R$, then
  \[
    \PP(X \le x, Y \le y)
    = \PP(X \le x) \PP(Y \le y),
  \]
  i.e. the events $\{X \le x\}$ and $\{Y \le y\}$ are independent.
\end{proof}

\begin{definition}
  The pair of random variables $(X, Y)$
  is said to be \emph{jointly (absolutely continuous)}
  if its joint cdf can be written as
  \[
    F_{(X, Y)}(x, y) = \int_{-\infty}^x \int_{-\infty}^y f_{(X, Y)}(u, v) \, du dv
  \]
  for all $x, y \in \R$ and
  $f_{(X, Y)} : \R^2 \to [0, \infty)$. If this is the
  case, then $f_{(X, Y)}$ is called the \emph{joint pdf}
  of $(X, Y)$.
\end{definition}

\begin{remark}
  We have
  \[
    \frac{\partial^2}{\partial x \partial y} F_{(X, Y)}(x, y) = f_{(X, Y)}(x, y)
  \]
  if this derivative exists at $(x, y)$. If it
  does not exist then we set $f_{X, Y}(x, y) = 0$ (this is fine since an absolutely continuous function is
  differentiable \emph{almost everywhere}, i.e.
  except on a set of measure zero).
\end{remark}

\begin{remark}
  In the context of densities, we have for
  $A \subseteq \R^2$ (which is ``nice enough'') that
  \[
    \PP((X, Y) \in A) = \iint_A f_{(X, Y)}(u, v) \, du dv.
  \]
  Here the ``nice enough'' sets are the ones which
  can be built up from rectangles.
\end{remark}

\begin{definition}
  Let $(X, Y)$ be a continuous random vector
  with joint pdf $f_{(X, Y)}$. The \emph{marginal pdf}
  of $X$, denoted by $f_X$ is given by
  \[
    f_X(x) = \int_{-\infty}^\infty f_{(X, Y)}(x, y) \, dy.
  \]
  The marginal pdf $f_Y$ of $Y$ is defined similarly,
  integrating out the $x$ variable instead.
\end{definition}

\begin{remark}
  Clearly $f_X$ (as defined above) is nonnegative
  and also
  \[
    \int_{-\infty}^\infty f_X(x) \, dx
    = \int_{-\infty}^\infty \int_{-\infty}^\infty f_{(X, Y)}(x, y) \, dy \, dx
    = 1.
  \]
  Thus $f_X$ is indeed a valid density function.
\end{remark}

\begin{prop}
  Let $X$ and $Y$ be jointly absolutely continuous
  random variables with joint pdf $f_{(X, Y)}$.
  Then $X$ and $Y$ are independent if and only if
  $f_{(X, Y)}$ can be expressed as
  \[
    f_{(X, Y)}(x, y) = g(x) h(y)
  \]
  for every $x, y \in \R$, where $g, h : \R \to [0, \infty)$.
\end{prop}

\begin{proof}
  $(\Rightarrow)$ By independence, we have
  $F_{(X, Y)}(x, y) = F_X(x) F_Y(y)$, so
  \begin{align*}
    f_{(X, Y)}(x, y)
    = \frac{\partial^2}{\partial x \partial y} F_{(X, Y)}(x, y)
    &= \frac{\partial}{\partial x} \left(\frac{\partial}{\partial y} F_X(x) F_Y(y)\right)
    = \frac{\partial}{\partial x} \left(F_X(x) \frac{\partial}{\partial y} F_Y(y)\right) \\
    &= \frac{\partial}{\partial x}\left(F_X(x) f_Y(y)\right)
    = f_X(x) f_Y(y).
  \end{align*}
  This gives the desired conclusion in this direction.

  $(\Leftarrow)$ Check this as an exercise.
\end{proof}

\begin{prop}
  Random variables $X, Y$ are independent if and
  only if
  \[\EE[h_1(X) h_2(Y)] = \EE[h_1(X)] \EE[h_2(Y)]\]
  for all $h_1, h_2 : \R \to \R$.
\end{prop}

\begin{proof}
  $(\Rightarrow)$ This is clear by the independence
  of $X$ and $Y$.

  $(\Leftarrow)$ As in the discrete case, use
  indicator functions. Then argue using limits
  of indicators.
\end{proof}

\begin{example}
  Let $X, Y$ be two random variables with joint pdf
  given by
  \[
    f_{(X, Y)}(x, y) = 2e^{-x - y},
    \quad 0 < x < y < \infty.
  \]
  Are $X$ and $Y$ independent? The answer is no.
  It might be tempting to say that $f_{(X, Y)}$
  factors, but it does not factor \emph{over all of $\R^2$}:
  The region where the above formula
  holds adds dependence between $x$ and $y$. To
  verify this, we can compute the marginal densities.
  We have
  \[
    f_X(x)
    = \int_x^\infty f_{(X, Y)}(x, y) \, dy
    = \int_x^\infty 2e^{-x} e^{-y} \, dy
    = 2e^{-x} \left[-e^{-y}\right]_{y = x}^{y = \infty}
    = 2e^{-2x}
  \]
  for $x > 0$. This says that
  $X \sim \Exp(2)$. Similarly, we can find that
  \[
    f_Y(y)
    = \int_0^y 2e^{-x} e^{-y} \, dx
    = 2e^{-y} \left[-e^{-x}\right]_{x = 0}^{x = y}
    = 2e^{-y}(-e^{-y} + 1)
    = 2e^{-y} - 2e^{-2y}
  \]
  for $y > 0$. Note that $f_Y(y) \ge 0$ since
  $1 - e^{-y} \ge 0$ for $y > 0$, and
  \[
    \int_0^\infty f_Y(y)\, dy
    = \int_0^\infty 2e^{-y}(-e^{-y} + 1) \, dy
    = 2 \int_0^\infty e^{-y} \, dy - \int_0^\infty 2e^{-2y} \, dy
    = 2 - 1 = 1,
  \]
  where we recognize the first integral as the integral
  of a standard exponential density and the second
  integral as the integral of an exponential density
  with parameter $\lambda = 2$. Now we can see that
  \[
    f_{(X, Y)}(x, y) = 2 e^{-x - y}
    \ne 2e^{-2x}(2e^{-y} - 2e^{-2y})
    = f_X(x) f_Y(y),
  \]
  which shows that $X$ and $Y$ are not independent.
\end{example}

\section{Transformations of Random Variables}
\begin{theorem}
  Let $X$ and $Y$ be two independent random variables
  with respective density functions $f_X$ and $f_Y$.
  Then $Z = X + Y$ has density function given by
  \[
    f_Z(z) = (f_X * f_Y)(z)
    = \int_{-\infty}^\infty f_X(x) f_Y(z - x) \, dx
  \]
  for all $z \in \R$.
\end{theorem}

\begin{proof}
  For fixed $z \in \R$, we have
  \[
    \PP(Z \le z) = \PP(X + Y \le z)
    = \iint_{A_z} f_{(X, Y)}(x, y) \, dx dy
    = \int_{-\infty}^\infty \int_{-\infty}^{z - x} f_X(x) f_Y(y) \, dy dx,
  \]
  where $A_z = \{(x, y) \in \R^2 : x + y \le z\}$.
  By substituting $u = x$ and $v = x + y$ (and
  exchanging the order of integration, which is
  permissible by Tonelli's theorem since $f_{(X, Y)}(x, y) \ge 0$), we get
  \[
    \PP(Z \le z)
    = \int_{-\infty}^z \int_{-\infty}^\infty f_{(X, Y)}(u, v - u) \, du dv
    = \int_{-\infty}^z \int_{-\infty}^\infty f_X(u) f_Y(v - u) \, du dv
  \]
  by the independence of $X, Y$. Now differentiating
  both sides in $z$ gives the desired result.
\end{proof}

\begin{theorem}
  Let $X$ and $Y$ be jointly absolutely continuous
  with joint density $f_{(X, Y)}$. Let
  \[
    D = \{(x, y) \in \R^2 : f_{(X, Y)}(x, y) > 0\}
    \quad \text{and} \quad
    T : (x, y) \to T(x, y) = (u(x, y), v(x, y)).
  \]
  Assume that $T$ is a bijection from
  $D$ to $S \subseteq \R^2$. Then the pair
  $(U, V) = (u(X, Y), v(X, Y))$ is jointly absolutely
  continuous with density function
  \[
    f_{(U, V)}(u, v) =
    \begin{cases}
      f_{(X, Y)}(x(u, v), y(u, v)) |J(u, v)| & \text{if $(u, v) \in S$}, \\
      0 & \text{otherwise},
    \end{cases}
  \]
  where $J(u, v)$ is the Jacobian determinant
  of $T^{-1}$ at $(u, v)$.
\end{theorem}

\begin{proof}
  Let $A \subseteq D$ and $T(A) = B$. Since
  $T$ is a bijection, we have
  \[
    \PP((U, V) \in B)
    = \PP(T(X, Y) \in B)
    = \PP((X, Y) \in A).
  \]
  Now by the change of variables formula from
  multivariable calculus, we have
  \begin{align*}
    \PP((U, V) \in B)
    = \PP((X, Y) \in A)
    &= \iint_A f_{(X, Y)}(x, y) \, dx dy \\
    &= \iint_B f_{(X, Y)}(x(u, v), y(u, v)) |J(u, v)| \, du dv.
  \end{align*}
  From this we obtain
  \[
    \iint_B f_{(U, V)}(u, v) \, du dv
    = \PP((U, V) \in B)
    = \iint_B f_{(X, Y)}(x(u, v), y(u, v)) |J(u, v)| \, du dv.
  \]
  Since this holds for every $B \subseteq S$, we
  deduce that the integrands must be equal (if they
  differed on a set $E$ of positive measure, then their
  integrals will differ on a subset of $E$).
\end{proof}

\section{Conditional Distributions}

\begin{definition}
  The \emph{conditional density} of $X$ given
  $\{Y = y\}$ is denoted by
  \[
    F_{X | Y}(x | y) = \frac{f_{(X, Y)}(x, y)}{f_Y(y)}
    = \frac{f_{(X, Y)}(x, y)}{\displaystyle \int_{-\infty}^\infty f_{(X, Y)}(x, y) \, dx}
  \]
   for all $y \in \R$ with $f_Y(y) > 0$.
\end{definition}

\begin{example}
  Let
  \[
    f_{(X, Y)}(x, y) =
    \begin{cases}
      2e^{-x - y} & \text{if $0 < x < y < \infty$}, \\
      0 & \text{otherwise}.
    \end{cases}
  \]
  Then we have
  \[
    f_{Y | X}(y | x)
    = \frac{f_{(X, Y)}(x, y)}{f_X(x)}
    = \frac{2e^{-x - y}}{2e^{-2x}} = e^{x - y}
    \quad \text{$0 < x < y < \infty$}
  \]
  and
  \[
    f_{X | Y}(x | y) = \frac{f_{(X, Y)}(x, y)}{f_Y(y)}
    = \frac{2e^{-x - y}}{2e^{-y}(1 - e^{-y})}
    = \frac{e^{-x}}{1 - e^{-y}} \quad \text{$0 < x < y < \infty$},
  \]
  using the marginal pdfs we computed from a previous
  example.
\end{example}

\begin{definition}
  The \emph{conditional expectation} of $X$ given
  $\{Y = y\}$ is defined by
  \[
    \EE[X | Y = y]
    = \int_{-\infty}^\infty x f_{X | Y}(x | y) \, dx
    = \int_{-\infty}^\infty x \frac{f_{(X, Y)}(x, y)}{f_Y(y)} \, dx
  \]
  for all $y \in \R$ with $f_Y(y) > 0$.
\end{definition}

\begin{prop}
  For continuous random variables $X, Y$ with
  density functions $f_X, f_Y$, we have
  \[
    \EE[X] =
    \int_{-\infty}^\infty \EE[X | Y = y] f_Y(y) \, dy
    \quad \text{and} \quad
    \EE[Y] =
    \int_{-\infty}^\infty \EE[Y | X = x] f_X(x) \, dx.
  \]
\end{prop}

\begin{proof}
  We have
  \begin{align*}
    \int_{-\infty}^\infty \EE[X | Y = y] f_Y(y) \, dy
    &= \int_{-\infty}^\infty \int_{-\infty}^\infty x \frac{f_{(X, Y)}(x, y)}{f_Y(y)} f_Y(y) \, dy \\
    &= \int_{-\infty}^\infty \int_{-\infty}^\infty x f_{(X, Y)}(x, y) \, dy = \EE[X].
  \end{align*}
  The computation for $\EE[Y]$ is identical.
\end{proof}

\section{Bivariate Normal Distribution}

\begin{definition}
  The random vector $(X, Y)$ is said to be
  \emph{normally distributed} if
  \[
    f_{(X, Y)}(x, y) = \frac{1}{2\pi \sqrt{1 - \rho^2}}
    \exp\left(-\frac{1}{2(1 - \rho^2)} (x^2 - 2\rho xy + y^2)\right)
  \]
  for all $x, y \in \R$ and $\rho \in (-1, 1)$.
\end{definition}

\begin{remark}
  Note that if $\rho = 0$, then the joint density
  becomes
  \[
    f_{(X, Y)}(x, y) = \frac{1}{2\pi} \exp\left(-\frac{1}{2}(x^2 + y^2)\right)
    = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2}
    \cdot \frac{1}{\sqrt{2\pi}} e^{-y^2 / 2}
  \]
  for every $x, y \in \R$. So $X, Y$ are
  independent and $X, Y \sim \mathcal{N}(0, 1)$.
\end{remark}

\begin{example}
  We can compute the marginal distribution of $X$ by
  \begin{align*}
    f_X(x)
    = \int_{-\infty}^\infty f_{(X, Y)}(x, y)\, dxdy
    &= \frac{1}{2\pi \sqrt{1 - \rho^2}} \int_{-\infty}^\infty e^{-(x^2 - 2\rho xy + y^2) / 2(1 - \rho^2)} \, dy \\
    &= \frac{1}{2\pi\sqrt{1 - \rho^2}} \int_{-\infty}^\infty e^{-((y - \rho x)^2 + x^2(1 - \rho^2)) / 2 (1 - \rho^2)}\, dy \\
    &= \frac{1}{2\pi\sqrt{1 - \rho^2}} e^{-x^2(1 - \rho^2) / 2(1 - \rho^2)} \int_{-\infty}^\infty e^{-(y - \rho x)^2 / 2 (1 - \rho^2)}\, dy \\
    &= \frac{e^{-x^2 / 2}}{\sqrt{2\pi}} \cdot \frac{1}{\sqrt{2\pi(1 - \rho^2)}} \int_{-\infty}^\infty e^{-(y - \rho x)^2 / 2(1 - \rho^2)}\, dy
    = \frac{1}{\sqrt{2\pi}} e^{-x^2 / 2},
  \end{align*}
  since the last integral is the integral of the pdf
  of a $\mathcal{N}(\rho x, 1 - \rho^2)$ random
  variable. From this we can recognize that
  $X \sim \mathcal{N}(0, 1)$, and by symmetry
  we also see that $Y \sim \mathcal{N}(0, 1)$.
\end{example}

\begin{example}
  Now we compute the conditional pdf of $Y$
  given $X = x$. This is
  \begin{align*}
    f_{Y | X}(y | x)
    = \frac{f_{(X, Y)}(x, y)}{f_X(x)}
    = \frac{e^{-((y - \rho^2) + x^2(1 - \rho^2)) / 2(1 - \rho^2)}}{\sqrt{2\pi} \sqrt{2\pi (1 - \rho^2)}} 
    \cdot \frac{\sqrt{2\pi}}{e^{-x^2 / 2}}
    &= \frac{e^{-(y - \rho x)^2 / 2(1 - \rho^2)}}{\sqrt{2\pi(1 - \rho^2)}}.
  \end{align*}
  This shows that $Y | X = x$ has a normal
  density with mean $\rho x$ and variance $1 - \rho^2$.
  We also get the formula for $f_{X | Y}(x | y)$
  by symmetry by exchanging the roles of $x$ and $y$.
  Note that this also gives
  \[
    \EE[Y | X = x] = \rho x \quad \text{and} \quad
    \EE[X | Y = y] = \rho y.
  \]
  Note that the conditional expectation of
  $X$ given $Y = y$ is linear in $y$ (and linear
  in $x$ for $Y | X = x$).
\end{example}

\begin{example}
  We compute $\EE[XY]$. We can write
  \[
    \EE[XY] = \int_{-\infty}^\infty \EE[XY | X = x] f_X(x)\, dx.
  \]
  Since we condition on $X = x$, we have
  $\EE[XY | X = x] = \EE[xY | X = x] = x \EE[Y | X = x]$, and so
  \[
    \EE[XY] = \int_{-\infty}^\infty x \EE[Y | X = x] f_X(x)\, dx
    = \int_{\infty}^\infty x (\rho x) f_X(x)\, dx
    = \rho \int_{-\infty}^\infty x^2 f_X(x)\, dx
    = \rho
  \]
  since we recognize the last integral as
  the integral for $\Var[X] = 1$.
\end{example}

\begin{definition}
  The \emph{covariance} of two random variables
  $X$ and $Y$ is
  defined by
  \[
    \Cov(X, Y)
    = \EE[(X - \EE[X])(Y - \EE[Y])]
    = \EE[XY] - \EE[X] \EE[Y].
  \]
\end{definition}

\begin{remark}
  The above computations show that
  \[
    \Cov(X, Y) = \EE[XY] - \EE[X] \EE[Y] = \rho - 0 \cdot 0 = \rho.
  \]
  So the parameter $\rho$ in the bivariate normal
  pdf is the covariance of $X$ and $Y$.
\end{remark}

\begin{remark}
  If $(X, Y)$ is a normal random vector, then
  $X$ and $Y$ are independent \emph{if and only if}
  $X$ and $Y$ are uncorrelated, i.e. $\Cov(X, Y) = 0$.
  Recall that being uncorrelated does not
  imply independence in general. This special case
  works because the conditional expectations are
  linear.
\end{remark}

\begin{definition}
  The \emph{correlation coefficient} of two random
  variables $X$ and $Y$ is given by
  \[
  \rho(X, Y) = \frac{\Cov(X, Y)}{\sqrt{\Var[X]} \sqrt{\Var[Y]}},
  \]
  provided that $\Var[X], \Var[Y] > 0$, i.e.
  $X \ne \EE[X]$ and $Y \ne \EE[Y]$.
\end{definition}

\begin{definition}
  Let $\mu \in \R^d$ and $\Sigma$ be a
  positive definite symmetric $d \times d$ matrix.
  We say that $X$ has the multivariate
  normal distribution with mean $\mu$ and
  covariance matrix $\Sigma$, written
  $X \sim \mathcal{N}(\mu, \Sigma)$, if
  \[
    f_X(x) = \frac{1}{(2\pi)^{d / 2} \sqrt{\det \Sigma}} \exp\left(-\frac{1}{2}(x - \mu)^T \Sigma^{-1} (x - \mu)\right)
    \quad \text{for all $x \in \R^d$}.
  \]
\end{definition}

\begin{remark}
  When $d = 2$, we get
  \[
    f_X(x) = \frac{1}{2\pi \sqrt{\sigma_1^2 \sigma_2^2 (1 - \rho^2)}}
    \exp\left(\frac{-1}{2(1 - \rho^2)} \left[\frac{(x_1 - \mu_1)^2}{\sigma_1^2} - \frac{2\rho(x_1 - \mu_1)(x_2 - \mu_2)}{\sigma_1 \sigma_2} + \frac{(x_2 - \mu_2)^2}{\sigma_2^2}\right]\right)
  \]
  as the joint pdf. Here we have
  \[
    x =
    \begin{bmatrix}
      x_1 \\ x_2
    \end{bmatrix}, \quad
    \mu =
    \begin{bmatrix}
      \mu_1 \\ \mu_2
    \end{bmatrix}, \quad \text{and} \quad
    \Sigma =
    \begin{bmatrix}
      \sigma_1^2 & \rho \sigma_1 \sigma_2 \\
      \rho \sigma_1 \sigma_2 & \sigma_2^2
    \end{bmatrix}.
  \]
  If $X = (X_1, X_2)$, then $\EE[X_1] = \mu_1$,
  $\EE[X_2] = \mu_2$, $\Var[X_1] = \sigma_1^2$,
  $\Var[X_2] = \sigma_2^2$, and $\rho(X_1, X_2) = \rho$.
\end{remark}

\section{Homework Problems}
Problems \#1, 3, 4, 9, 11, 15, 20, 21, 24, 26
from Grimmett and Welsh.
