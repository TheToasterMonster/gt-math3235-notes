\chapter{Multivariate Continuous Distributions}

\section{Multivariate Absolutely Continuous Distributions}
\begin{definition}
  Let $(X, Y)$ be a two-dimensional random vector,
  its \emph{distribution function} is given by
  \[
    F_{(X, Y)}(x, y) = \PP(X \leq x, Y \leq y)
    \quad \text{for $(x, y) \in \R^2$}.
  \]
\end{definition}

\begin{prop}
  We have the following properties for the
  distribution function of $(X, Y)$:
  \begin{enumerate}[(i)]
    \item $\lim_{x \to -\infty} F_{(X, Y)}(x, y) = 0$
      and $\lim_{y \to -\infty} F_{(X, Y)}(x, y) = 0$.
    \item $\lim_{x, y \to \infty} F_{(X, Y)}(x, y) = 1$.
    \item $F_{(X, Y)}$ is weakly increasing
      in each variable: If $x_1 \le x_2$ then
      $F_{(X, Y)}(x_1, y) \le F_{(X, Y)}(x_2, y)$
      and if $y_1 \le y_2$ then
      $F_{(X, Y)}(x, y_1) \le F_{(X, Y)}(x, y_2)$.
    \item If $x_1 \le y_1$ and $x_2 \le y_2$, then\footnote{One can interpret this integral as a \emph{Riemann-Stieltjes integral}.}
      \[
        \int_{x_1}^{x_2} \int_{y_1}^{y_2} dF(u, v)
        \ge 0.
      \]
  \end{enumerate}
\end{prop}

\begin{proof}
  $(i)$ We have
  \begin{align*}
    \lim_{x \to -\infty} F_{(X, Y)}(x, y)
    &= \lim_{x \to -\infty} \PP(X^{-1}((-\infty, x])
    \cap Y^{-1}((-\infty, y])) \\
    &= \PP(Y^{-1}((-\infty, y]) \cap \bigcap_{x \in \R} X^{-1}((-\infty, x]) \\
    &= \PP(Y^{-1}((-\infty, y]) \cap \varnothing)
    = 0.
  \end{align*}
  $(ii)$ We have
  \[
    \lim_{x \to \infty} F_{(X, Y)}(x, y)
    = \PP(\{X \le \infty\} \cap \{Y \le y\})
    = \PP(Y \le y) = F_Y(y).
  \]
  Then taking $y \to \infty$ gives
  $\lim_{y \to \infty} F_Y(y) = 1$.

  $(iii)$ If $x_1 \le x_2$ and $y$ is fixed, then
  $\PP(X \le x_1, Y \le y) \le \PP(X \le x_2, Y \le y)$ since
  \[
    X^{-1}((-\infty, x_1]) \cap Y^{-1}((-\infty, y])
    \subseteq X^{-1}((-\infty, x_2]) \cap Y^{-1}((-\infty, y]).
  \]

  $(iv)$ The integral corresponds to $\PP(X \in (x_1, x_2), Y \in (y_1, y_2))$,
  which must be nonnegative.
\end{proof}

\begin{remark}
  The above properties characterize distribution
  functions of two-dimensional random vectors.
\end{remark}

\section{Independence of Continuous Random Variables}
\begin{definition}
  Two random variables $X, Y$ are said to be
  \emph{independent}
  if for all $x, y \in \R$, the events
  $\{X \le x\}$ and $\{Y \le y\}$ are independent.
\end{definition}

\begin{prop}
  Two continuous random variables $X, Y$ are independent
  if and only if \[
    F_{(X, Y)}(x, y) = F_X(x) F_Y(y)
  \]
  for every $x, y \in \R$.
\end{prop}

\begin{proof}
  $(\Rightarrow)$ If $X$ and $Y$ are independent,
  then $\{X \le x\}$ and $\{Y \le y\}$ are independent
  for all $x, y \in \R$, so
  \[
    \PP(\{X \le x\} \cap \{Y \le y\}) = \PP(\{X \le x\}) \PP(\{Y \le y\}),
  \]
  i.e. we have $F_{(X, Y)}(x, y) = F_X(x) F_Y(y)$.

  $(\Leftarrow)$ If $F_{(X, Y)}(x, y) = F_X(x) F_Y(y)$,
  then for all $x, y \in \R$, then
  \[
    \PP(X \le x, Y \le y)
    = \PP(X \le x) \PP(Y \le y),
  \]
  i.e. the events $\{X \le x\}$ and $\{Y \le y\}$ are independent.
\end{proof}

\begin{definition}
  The pair of random variables $(X, Y)$
  is said to be \emph{jointly (absolutely continuous)}
  if its joint cdf can be written as
  \[
    F_{(X, Y)}(x, y) = \int_{-\infty}^x \int_{-\infty}^y f_{(X, Y)}(u, v) \, du dv
  \]
  for all $x, y \in \R$ and
  $f_{(X, Y)} : \R^2 \to [0, \infty)$. If this is the
  case, then $f_{(X, Y)}$ is called the \emph{joint pdf}
  of $(X, Y)$.
\end{definition}

\begin{remark}
  We have
  \[
    \frac{\partial^2}{\partial x \partial y} F_{(X, Y)}(x, y) = f_{(X, Y)}(x, y)
  \]
  if this derivative exists at $(x, y)$. If it
  does not exist then we set $f_{X, Y}(x, y) = 0$ (this is fine since an absolutely continuous function is
  differentiable \emph{almost everywhere}, i.e.
  except on a set of measure zero).
\end{remark}

\begin{remark}
  In the context of densities, we have for
  $A \subseteq \R^2$ (which is ``nice enough'') that
  \[
    \PP((X, Y) \in A) = \iint_A f_{(X, Y)}(u, v) \, du dv.
  \]
  Here the ``nice enough'' sets are the ones which
  can be built up from rectangles.
\end{remark}

\begin{definition}
  Let $(X, Y)$ be a continuous random vector
  with joint pdf $f_{(X, Y)}$. The \emph{marginal pdf}
  of $X$, denoted by $f_X$ is given by
  \[
    f_X(x) = \int_{-\infty}^\infty f_{(X, Y)}(x, y) \, dy.
  \]
  The marginal pdf $f_Y$ of $Y$ is defined similarly,
  integrating out the $x$ variable instead.
\end{definition}

\begin{remark}
  Clearly $f_X$ (as defined above) is nonnegative
  and also
  \[
    \int_{-\infty}^\infty f_X(x) \, dx
    = \int_{-\infty}^\infty \int_{-\infty}^\infty f_{(X, Y)}(x, y) \, dy \, dx
    = 1.
  \]
  Thus $f_X$ is indeed a valid density function.
\end{remark}

\begin{prop}
  Let $X$ and $Y$ be jointly absolutely continuous
  random variables with joint pdf $f_{(X, Y)}$.
  Then $X$ and $Y$ are independent if and only if
  $f_{(X, Y)}$ can be expressed as
  \[
    f_{(X, Y)}(x, y) = g(x) h(y)
  \]
  for every $x, y \in \R$, where $g, h : \R \to [0, \infty)$.
\end{prop}

\begin{proof}
  $(\Rightarrow)$ By independence, we have
  $F_{(X, Y)}(x, y) = F_X(x) F_Y(y)$, so
  \begin{align*}
    f_{(X, Y)}(x, y)
    = \frac{\partial^2}{\partial x \partial y} F_{(X, Y)}(x, y)
    &= \frac{\partial}{\partial x} \left(\frac{\partial}{\partial y} F_X(x) F_Y(y)\right)
    = \frac{\partial}{\partial x} \left(F_X(x) \frac{\partial}{\partial y} F_Y(y)\right) \\
    &= \frac{\partial}{\partial x}\left(F_X(x) f_Y(y)\right)
    = f_X(x) f_Y(y).
  \end{align*}
  This gives the desired conclusion in this direction.

  $(\Leftarrow)$ Check this as an exercise.
\end{proof}

\begin{prop}
  Random variables $X, Y$ are independent if and
  only if
  \[\EE[h_1(X) h_2(Y)] = \EE[h_1(X)] \EE[h_2(Y)]\]
  for all $h_1, h_2 : \R \to \R$.
\end{prop}

\begin{proof}
  $(\Rightarrow)$ This is clear by the independence
  of $X$ and $Y$.

  $(\Leftarrow)$ As in the discrete case, use
  indicator functions. Then argue using limits
  of indicators.
\end{proof}

\begin{example}
  Let $X, Y$ be two random variables with joint pdf
  given by
  \[
    f_{(X, Y)}(x, y) = 2e^{-x - y},
    \quad 0 < x < y < \infty.
  \]
  Are $X$ and $Y$ independent? The answer is no.
  It might be tempting to say that $f_{(X, Y)}$
  factors, but it does not factor \emph{over all of $\R^2$}:
  The region where the above formula
  holds adds dependence between $x$ and $y$. To
  verify this, we can compute the marginal densities.
  We have
  \[
    f_X(x)
    = \int_x^\infty f_{(X, Y)}(x, y) \, dy
    = \int_x^\infty 2e^{-x} e^{-y} \, dy
    = 2e^{-x} \left[-e^{-y}\right]_{y = x}^{y = \infty}
    = 2e^{-2x}
  \]
  for $x > 0$. This says that
  $X \sim \Exp(2)$. Similarly, we can find that
  \[
    f_Y(y)
    = \int_0^y 2e^{-x} e^{-y} \, dx
    = 2e^{-y} \left[-e^{-x}\right]_{x = 0}^{x = y}
    = 2e^{-y}(-e^{-y} + 1)
    = 2e^{-y} - 2e^{-2y}
  \]
  for $y > 0$. Note that $f_Y(y) \ge 0$ since
  $1 - e^{-y} \ge 0$ for $y > 0$, and
  \[
    \int_0^\infty f_Y(y)\, dy
    = \int_0^\infty 2e^{-y}(-e^{-y} + 1) \, dy
    = 2 \int_0^\infty e^{-y} \, dy - \int_0^\infty 2e^{-2y} \, dy
    = 2 - 1 = 1,
  \]
  where we recognize the first integral as the integral
  of a standard exponential density and the second
  integral as the integral of an exponential density
  with parameter $\lambda = 2$. Now we can see that
  \[
    f_{(X, Y)}(x, y) = 2 e^{-x - y}
    \ne 2e^{-2x}(2e^{-y} - 2e^{-2y})
    = f_X(x) f_Y(y),
  \]
  which shows that $X$ and $Y$ are not independent.
\end{example}

\section{Transformations of Random Variables}
\begin{theorem}
  Let $X$ and $Y$ be two independent random variables
  with respective density functions $f_X$ and $f_Y$.
  Then $Z = X + Y$ has density function given by
  \[
    f_Z(z) = (f_X * f_Y)(z)
    = \int_{-\infty}^\infty f_X(x) f_Y(z - x) \, dx
  \]
  for all $z \in \R$.
\end{theorem}

\begin{proof}
  For fixed $z \in \R$, we have
  \[
    \PP(Z \le z) = \PP(X + Y \le z)
    = \iint_{A_z} f_{(X, Y)}(x, y) \, dx dy
    = \int_{-\infty}^\infty \int_{-\infty}^{z - x} f_X(x) f_Y(y) \, dy dx,
  \]
  where $A_z = \{(x, y) \in \R^2 : x + y \le z\}$.
  By substituting $u = x$ and $v = x + y$ (and
  exchanging the order of integration, which is
  permissible by Tonelli's theorem since $f_{(X, Y)}(x, y) \ge 0$), we get
  \[
    \PP(Z \le z)
    = \int_{-\infty}^z \int_{-\infty}^\infty f_{(X, Y)}(u, v - u) \, du dv
    = \int_{-\infty}^z \int_{-\infty}^\infty f_X(u) f_Y(v - u) \, du dv
  \]
  by the independence of $X, Y$. Now differentiating
  both sides in $z$ gives the desired result.
\end{proof}

\begin{theorem}
  Let $X$ and $Y$ be jointly absolutely continuous
  with joint density $f_{(X, Y)}$. Let
  \[
    D = \{(x, y) \in \R^2 : f_{(X, Y)}(x, y) > 0\}
    \quad \text{and} \quad
    T : (x, y) \to T(x, y) = (u(x, y), v(x, y)).
  \]
  Assume that $T$ is a bijection from
  $D$ to $S \subseteq \R^2$. Then the pair
  $(U, V) = (u(X, Y), v(X, Y))$ is jointly absolutely
  continuous with density function
  \[
    f_{(U, V)}(u, v) =
    \begin{cases}
      f_{(X, Y)}(x(u, v), y(u, v)) |J(u, v)| & \text{if $(u, v) \in S$}, \\
      0 & \text{otherwise},
    \end{cases}
  \]
  where $J(u, v)$ is the Jacobian determinant
  of $T^{-1}$ at $(u, v)$.
\end{theorem}

\begin{proof}
  Let $A \subseteq D$ and $T(A) = B$. Since
  $T$ is a bijection, we have
  \[
    \PP((U, V) \in B)
    = \PP(T(X, Y) \in B)
    = \PP((X, Y) \in A).
  \]
  Now by the change of variables formula from
  multivariable calculus, we have
  \begin{align*}
    \PP((U, V) \in B)
    = \PP((X, Y) \in A)
    &= \iint_A f_{(X, Y)}(x, y) \, dx dy \\
    &= \iint_B f_{(X, Y)}(x(u, v), y(u, v)) |J(u, v)| \, du dv.
  \end{align*}
  From this we obtain
  \[
    \iint_B f_{(U, V)}(u, v) \, du dv
    = \PP((U, V) \in B)
    = \iint_B f_{(X, Y)}(x(u, v), y(u, v)) |J(u, v)| \, du dv.
  \]
  Since this holds for every $B \subseteq S$, we
  deduce that the integrands must be equal (if they
  differed on a set $E$ of positive measure, then their
  integrals will differ on a subset of $E$).
\end{proof}

\section{Conditional Distributions}

\begin{definition}
  The \emph{conditional density} of $X$ given
  $\{Y = y\}$ is denoted by
  \[
    F_{X | Y}(x | y) = \frac{f_{(X, Y)}(x, y)}{f_Y(y)}
    = \frac{f_{(X, Y)}(x, y)}{\displaystyle \int_{-\infty}^\infty f_{(X, Y)}(x, y) \, dx}
  \]
   for all $y \in \R$ with $f_Y(y) > 0$.
\end{definition}

\begin{example}
  Let
  \[
    f_{(X, Y)}(x, y) =
    \begin{cases}
      2e^{-x - y} & \text{if $0 < x < y < \infty$}, \\
      0 & \text{otherwise}.
    \end{cases}
  \]
  Then we have
  \[
    f_{Y | X}(y | x)
    = \frac{f_{(X, Y)}(x, y)}{f_X(x)}
    = \frac{2e^{-x - y}}{2e^{-2x}} = e^{x - y}
    \quad \text{$0 < x < y < \infty$}
  \]
  and
  \[
    f_{X | Y}(x | y) = \frac{f_{(X, Y)}(x, y)}{f_Y(y)}
    = \frac{2e^{-x - y}}{2e^{-y}(1 - e^{-y})}
    = \frac{e^{-x}}{1 - e^{-y}} \quad \text{$0 < x < y < \infty$},
  \]
  using the marginal pdfs we computed from a previous
  example.
\end{example}

\begin{definition}
  The \emph{conditional expectation} of $X$ given
  $\{Y = y\}$ is defined by
  \[
    \EE[X | Y = y]
    = \int_{-\infty}^\infty x f_{X | Y}(x | y) \, dx
    = \int_{-\infty}^\infty x \frac{f_{(X, Y)}(x, y)}{f_Y(y)} \, dx
  \]
  for all $y \in \R$ with $f_Y(y) > 0$.
\end{definition}

\begin{prop}
  For continuous random variables $X, Y$ with
  density functions $f_X, f_Y$, we have
  \[
    \EE[X] =
    \int_{-\infty}^\infty \EE[X | Y = y] f_Y(y) \, dy
    \quad \text{and} \quad
    \EE[Y] =
    \int_{-\infty}^\infty \EE[Y | X = x] f_X(x) \, dx.
  \]
\end{prop}

\begin{proof}
  We have
  \begin{align*}
    \int_{-\infty}^\infty \EE[X | Y = y] f_Y(y) \, dy
    &= \int_{-\infty}^\infty \int_{-\infty}^\infty x \frac{f_{(X, Y)}(x, y)}{f_Y(y)} f_Y(y) \, dy \\
    &= \int_{-\infty}^\infty \int_{-\infty}^\infty x f_{(X, Y)}(x, y) \, dy = \EE[X].
  \end{align*}
  The computation for $\EE[Y]$ is identical.
\end{proof}

\section{Homework Problems}
Problems \#1, 3, 4, 9, 11, 15, 20, 21, 24, 26
from Grimmett and Welsh.
